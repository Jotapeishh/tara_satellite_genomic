{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary exploration: reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this notebook is to study the data and compare what we get by calculating things and the corresponding values stored in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by picking one of the TARA matrices stored in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df1.columns and 'bbp_unc_443' in df1.columns:\n",
    "    df1 = df1.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df2.columns and 'bbp_unc_443' in df2.columns:\n",
    "    df2= df2.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1.columns == df2.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (df1.values - df2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = diff.sum()\n",
    "print(total_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err/df1.values.shape[0]*df1.values.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too high, so lets dig into that error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = diff.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(idxs[0])):\n",
    "    i = idxs[0][k]\n",
    "    j = idxs[1][k]\n",
    "    print(f\" Difference at ({i},{j}) = {diff[i,j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is only one 'big' error in the last element, meanwhile the rest can be attributed to float point. The numerical coordinates of the attribute are (79,31) as we can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[79].keys()[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problematic sample is `TSC280`, and the problematic feature is `PAR.par`. Looking at the individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Repo-stored value: {df1['PAR.par'].loc['TSC280']}.\\nNew obtained value: {df2['PAR.par'].loc['TSC280']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to recalculate this value 'by hand' and compare it to the two that we have. This will be done with the source code of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def find_satellite_file(directory, pattern):\n",
    "    regex = re.compile(pattern)\n",
    "    for file in os.listdir(directory):\n",
    "        if regex.match(file):\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "def select_n_nearest_valid(ds, feature, latitude, longitude, n):\n",
    "    latitudes = ds['lat'].values\n",
    "    longitudes = ds['lon'].values\n",
    "    \n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes, indexing='ij')\n",
    "    distances = np.sqrt((lat_grid - latitude)**2 + (lon_grid - longitude)**2)\n",
    "    \n",
    "    sorted_indices = np.unravel_index(np.argsort(distances, axis=None), distances.shape)\n",
    "    \n",
    "    valid_points = []\n",
    "    for i in range(len(sorted_indices[0])):\n",
    "        lat_idx = sorted_indices[0][i]\n",
    "        lon_idx = sorted_indices[1][i]\n",
    "        data_point = ds[feature].isel(lat=lat_idx, lon=lon_idx)\n",
    "        if not np.isnan(data_point.values):\n",
    "            valid_points.append(data_point.values)\n",
    "        if len(valid_points) >= n:\n",
    "            break\n",
    "    \n",
    "    if len(valid_points) > 0:\n",
    "        return np.mean(valid_points)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25 #number of near points.\n",
    "file_path = '../01_data/01_biological_data'\n",
    "file_name = 'metadata.tsv'\n",
    "sat_data_path = '../01_data/00_satellite_data'\n",
    "\n",
    "output_dir = '../01_data/02_satellite_data_processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file = os.path.join(file_path, file_name)\n",
    "md = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "md_srf = md[md.Layer == 'SRF'].copy()\n",
    "md_srf['Event.date.YYYYMM'] = md_srf['Event.date'].str[:7].str.replace('-', '')\n",
    "md_srf['Event.date.YYYYMM01'] = md_srf['Event.date'].str[:7].str.replace('-', '')+'01'\n",
    "\n",
    "satellite_features = [\n",
    "    'CHL.chlor_a', 'FLH.nflh', 'FLH.ipar', 'IOP.adg_unc_443', 'IOP.adg_443',\n",
    "    'IOP.aph_unc_443', 'IOP.aph_44', 'IOP.bbp_s', 'IOP.adg_s', 'bbp_unc_443', \n",
    "    'IOP.bbp_443', 'IOP.a_412', 'IOP.a_443', 'IOP.a_469', 'IOP.a_488', 'IOP.a_531', \n",
    "    'IOP.a_547', 'IOP.a_555', 'IOP.a_645', 'IOP.a_667', 'IOP.a_678', 'IOP.bb_412', \n",
    "    'IOP.bb_443', 'IOP.bb_469', 'IOP.bb_488', 'IOP.bb_531', 'IOP.bb_547', 'IOP.bb_555', \n",
    "    'IOP.bb_645', 'IOP.bb_667', 'IOP.bb_678', 'KD.Kd_490', 'NSST.sst', 'PAR.par', \n",
    "    'PIC.pic', 'POC.poc', 'RRS.aot_869', 'RRS.angstrom', 'RRS.Rrs_412', 'RRS.Rrs_443', \n",
    "    'RRS.Rrs_469', 'RRS.Rrs_488', 'RRS.Rrs_531', 'RRS.Rrs_547', 'RRS.Rrs_555', \n",
    "    'RRS.Rrs_645', 'RRS.Rrs_667', 'RRS.Rrs_678', 'SST.sst'\n",
    "]\n",
    "\n",
    "satellite_data_terra = pd.DataFrame(index=md_srf.index, columns=satellite_features)\n",
    "satellite_data_aqua = pd.DataFrame(index=md_srf.index, columns=satellite_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = md_srf.loc['TSC280'] #We use the obtained key.\n",
    "feature = 'PAR.par' #We use the obtained feature.\n",
    "latitude = row['Latitude']\n",
    "longitude = row['Longitude']\n",
    "date = row['Event.date.YYYYMM']\n",
    "resolution = '9km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_terra = rf\"TERRA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_terra = find_satellite_file(sat_data_path, pattern_terra)\n",
    "pattern_aqua = rf\"AQUA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_aqua = find_satellite_file(sat_data_path, pattern_aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_terra = xr.open_dataset(file_path_terra)\n",
    "ds_aqua = xr.open_dataset(file_path_aqua)\n",
    "terra_values = []\n",
    "aqua_values = []\n",
    "try:\n",
    "    variable_name = feature.split('.')[1]\n",
    "    \n",
    "    data_point_terra = select_n_nearest_valid(ds_terra, variable_name, latitude, longitude, 25)\n",
    "    terra_values.append(data_point_terra)\n",
    "\n",
    "    data_point_aqua = select_n_nearest_valid(ds_aqua, variable_name, latitude, longitude, 25)\n",
    "    aqua_values.append(data_point_aqua)\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Feature {feature} not found in dataset\")\n",
    "finally:\n",
    "    ds_terra.close()\n",
    "    ds_aqua.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_arr = np.array(aqua_values)\n",
    "terra_arr = np.array(terra_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr = (aqua_arr + terra_arr)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Value by hand: {final_arr[0]}.\\nValue stored in repo: {df1['PAR.par'].loc['TSC280']}. \\nNew value obtained by script: {df2['PAR.par'].loc['TSC280']}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the value obtained by hand is closer to the '_new' value than to the one stored in the repo, with error likely of approximation. The difference of the old value and the new value is of order $10^{-3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us make a simmilar study to the `n=1` case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01.tsv\", sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df3.columns and 'bbp_unc_443' in df3.columns:\n",
    "    df3 = df3.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df4.columns and 'bbp_unc_443' in df4.columns:\n",
    "    df4= df4.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.columns == df4.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = (df3.values - df4.values)\n",
    "print(diff2.max())\n",
    "print(diff2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err2 = diff2.sum()\n",
    "print(total_err2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the values are all equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the impression that the issue is somewhere between the part where the closest sat reads (physicall distance-wise) are chosen. There is a pending plan of testing with a third run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate further one may plot the points selected in the proccess for both versions and check if they are the same or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be related to eventual changes to the original database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: para mie. 6/nov\n",
    "Repetir todo el proceso de clustering con TARA Chile desde cero. De momento esto contempla:\n",
    "* Recuperar el `metadata_chile.tsv`.\n",
    "* Re-escribir el script `00_00_extract_satellite_data.py` porque cambian varias keys.\n",
    "* Unificar formato de fecha en el `metadata_chile.tsv`.\n",
    "\n",
    "## Cosas tangenciales a las que ponerle tiempo si es posible.\n",
    "* Estandarizar una pipeline que tome las keys como inputs.\n",
    "* Documentar los módulos y funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster projection on map\n",
    "We need to study the behavior of the drawing code on the new data, since it has different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = md.join(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_on_map(merged_data, cluster_column):\n",
    "    filtered_data = merged_data[~merged_data[cluster_column].isna()] # filter out rows where cluster_column is NaN\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = axes.flatten()  # flatten the array of axes for easy iteration\n",
    "    \n",
    "    plot_titles = [\n",
    "        f'Clusters Projection: {cluster_column}',\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    data_columns = [\n",
    "        cluster_column,\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    \n",
    "    unique_clusters = filtered_data[cluster_column].unique()\n",
    "    num_clusters = len(unique_clusters)\n",
    "    marker_styles = ['o', 's', '^', 'v', '<', '>', 'd', 'p', 'h', 'H', '*', 'x', '+', 'D']\n",
    "    if num_clusters > len(marker_styles):\n",
    "        marker_styles = (marker_styles * ((num_clusters // len(marker_styles)) + 1))[:num_clusters]\n",
    "    cluster_marker_map = dict(zip(unique_clusters, marker_styles))\n",
    "    \n",
    "    env_vars = ['Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']\n",
    "    norms = {}\n",
    "    \n",
    "    for data_column in env_vars:\n",
    "        vmin = filtered_data[data_column].min()\n",
    "        vmax = filtered_data[data_column].max()\n",
    "        norms[data_column] = Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.set_extent([-80, -67, -55,-17])\n",
    "\n",
    "        ax.add_feature(cfeature.LAND)\n",
    "        ax.add_feature(cfeature.OCEAN)\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        \n",
    "        ax.set_title(plot_titles[idx])\n",
    "        \n",
    "        data_column = data_columns[idx]\n",
    "        plot_data = filtered_data[~filtered_data[data_column].isna()]\n",
    "        \n",
    "        if idx == 0:\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    label=f'Cluster {cluster_id}',\n",
    "                    s=35,\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            ax.legend(loc='upper left')\n",
    "        else:\n",
    "            norm = norms[data_column]\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                sc = ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    c=cluster_points[data_column],\n",
    "                    s=35,\n",
    "                    cmap='viridis',\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    edgecolors='black',\n",
    "                    norm=norm,\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.5)\n",
    "            cbar.set_label(data_column)\n",
    "            handles = []\n",
    "            for cluster_id in unique_clusters:\n",
    "                marker = cluster_marker_map[cluster_id]\n",
    "                handle = plt.Line2D([], [], color='black', marker=marker, linestyle='', markersize=8, label=f'Cluster {cluster_id}')\n",
    "                handles.append(handle)\n",
    "            ax.legend(handles=handles, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'clusters_{cluster_column}.pdf')\n",
    "    plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mañana: doble plan de acción.\n",
    "* Tomar los clusters viejos y de ahí filtrar y mapear. ('Más fácil capaz') (esto ya está básicamente hechp)\n",
    "* Filtrar, clusterizar, y mapear. (Aprovecha que se quita una 'variable' de la clusterización):\n",
    "    * En vez de leer las matrices de clusters, leer las matrices de data bio\n",
    "* Comparar como lucen\n",
    "\n",
    "La nomenclatura será, en el mismo orden:\n",
    "* `_cf`: clustered filtered.\n",
    "* `_fc`: filtered clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Cluster -> Filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation.\n",
    "This code right ahead takes all the already clustered data, and saves a file of the selection of those samples that are 'SRF'. If a file named `kmeans_results_ch_srf_clustered_filtered.tsv` exists already in the folder, there is no need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch.tsv'\n",
    "\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in matrix_files:\n",
    "    pth = f\"{input_dir}/{file}\"\n",
    "    #bio_mtrx es en verdad cluster_mtrx. Arreglar para legibilidad.\n",
    "    clstr_mtrx =  pd.read_csv(file_path, sep='\\t', index_col=0) \n",
    "    meta_mtrx = pd.read_csv('../01_data/01_biological_data/metadata_chile.tsv', sep='\\t', index_col=0) \n",
    "    meta_mtrx = meta_mtrx[meta_mtrx['Depth level']== 'SRF']\n",
    "    dirty_df = meta_mtrx.join(clstr_mtrx)\n",
    "    clean_df = dirty_df.drop(meta_mtrx.columns, axis=1)\n",
    "    new_keys = {col: col+'_cf' for col in clean_df.columns}\n",
    "    clean_df.rename(columns = new_keys, inplace=True)\n",
    "    output_filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "    clean_df.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Filtro -> Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "We start by collecting the bio data, filtering only the SRF samples, and saving the resulting matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = '../01_data/01_biological_data'\n",
    "md_filename = 'metadata_chile.tsv'\n",
    "md_path = os.path.join(md_dir,md_filename)\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n",
    "\n",
    "output_dir = md_dir \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_clean = md[md['Depth level']=='SRF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(md_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in matrix_files:\n",
    "    print(f\"filtering {name}\")\n",
    "    file_path = os.path.join(md_dir, name)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    clean_matrix = md_clean.join(matrix).drop(md_clean.columns,axis = 1)\n",
    "    output_filename =  name[:-8] + '_srf.tsv'\n",
    "    clean_matrix.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "all_metrics_results = []\n",
    "clustering_results_dict = {}\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('_srf.tsv')])\n",
    "\n",
    "# Perform K-Means for different n-clusters for each matrix\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "for matrix_file in matrix_files:\n",
    "    print(f\"performing k-means to {matrix_file}\")\n",
    "    file_path = os.path.join(input_dir, matrix_file)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    matrix_type_subsample = \"_\".join(base_filename.split('_')[3:])\n",
    "    \n",
    "    perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "    # CLR normalized matrix clustering\n",
    "    clr_matrix = clr_(matrix)\n",
    "    perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "\n",
    "\n",
    "combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "#combined_clustering_results = combined_clustering_results.sort_index(axis=1)\n",
    "\n",
    "# Results of the kmeans\n",
    "output_filename = 'kmeans_results_ch_fc.tsv'\n",
    "combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "# Results of the metrics of the kmeans clustering\n",
    "metrics_df = pd.DataFrame(all_metrics_results)\n",
    "metrics_output_filename = 'kmeans_metrics_ch_fc.tsv'\n",
    "metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "# Plot metrics\n",
    "unique_matrices = metrics_df['matrix'].unique()\n",
    "for matrix_type_subsample in unique_matrices:\n",
    "    matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "    ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "    ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "    ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "    ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "    ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f'kmeans_metrics_{matrix_type_subsample}_ch_fc.pdf'\n",
    "    plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_fc.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "new_keys = {col: col+'_fc' for col in clusters.columns}\n",
    "clusters.rename(columns = new_keys, inplace=True)\n",
    "\n",
    "\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Tara Chile\n",
    "In this section we conduct some experiments in order to prepare the XGB study on the Chilean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we range over some selections of hyper-pareters for the XGB method, and use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for file in predictor_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    idx = f\"{file_name}_s{n_splits}_r{n_repeats}\"\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "    aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "\n",
    "    for target_column in columns_to_use:\n",
    "        n_clusters = int(target_column[-1])\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "\n",
    "        y_encoded = le.fit_transform(y)\n",
    "\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[idx, target_column] = (avg_accuracy, avg_f1_macro)\n",
    "                    \n",
    "#print(results_df)\n",
    "\n",
    "\n",
    "#results_df.to_csv('../03_results/out_predictions/predictions_kmeans.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clr_M0_all_kmeans_5</th>\n",
       "      <th>clr_M0_all_kmeans_6</th>\n",
       "      <th>clr_M0_all_kmeans_7</th>\n",
       "      <th>clr_M0_all_kmeans_8</th>\n",
       "      <th>clr_M1_all_kmeans_5</th>\n",
       "      <th>clr_M1_all_kmeans_6</th>\n",
       "      <th>clr_M1_all_kmeans_7</th>\n",
       "      <th>clr_M1_all_kmeans_8</th>\n",
       "      <th>clr_guidi_all_kmeans_5</th>\n",
       "      <th>clr_guidi_all_kmeans_6</th>\n",
       "      <th>clr_guidi_all_kmeans_7</th>\n",
       "      <th>clr_guidi_all_kmeans_8</th>\n",
       "      <th>clr_salazar_all_kmeans_5</th>\n",
       "      <th>clr_salazar_all_kmeans_6</th>\n",
       "      <th>clr_salazar_all_kmeans_7</th>\n",
       "      <th>clr_salazar_all_kmeans_8</th>\n",
       "      <th>clr_stress_all_kmeans_5</th>\n",
       "      <th>clr_stress_all_kmeans_6</th>\n",
       "      <th>clr_stress_all_kmeans_7</th>\n",
       "      <th>clr_stress_all_kmeans_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>matrix_tara_chile_adj_grids_25_all.tsv</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matrix_tara_chile_adj_grids_25_all.tsv_s8_r9</th>\n",
       "      <td>(0.4974415204678363, 0.4496015840162092)</td>\n",
       "      <td>(0.3682383040935673, 0.3513820496334686)</td>\n",
       "      <td>(0.3361111111111111, 0.40349933517825676)</td>\n",
       "      <td>(0.3441154970760234, 0.3258721698825865)</td>\n",
       "      <td>(0.35164473684210523, 0.33349314628726395)</td>\n",
       "      <td>(0.33903508771929824, 0.35536506960118075)</td>\n",
       "      <td>(0.34755116959064325, 0.3363500677401237)</td>\n",
       "      <td>(0.26593567251461986, 0.27121784188531944)</td>\n",
       "      <td>(0.29780701754385963, 0.277132747426865)</td>\n",
       "      <td>(0.33464912280701753, 0.3137653253155704)</td>\n",
       "      <td>(0.336951754385965, 0.328112584064965)</td>\n",
       "      <td>(0.28706140350877196, 0.2701996402517236)</td>\n",
       "      <td>(0.4296052631578948, 0.40274108017980564)</td>\n",
       "      <td>(0.44232456140350873, 0.39988779224890336)</td>\n",
       "      <td>(0.33888888888888885, 0.3026676916557869)</td>\n",
       "      <td>(0.354422514619883, 0.28794279601571265)</td>\n",
       "      <td>(0.3914473684210526, 0.3113786463889663)</td>\n",
       "      <td>(0.3901315789473684, 0.2715706070587606)</td>\n",
       "      <td>(0.3600877192982456, 0.27114084324051646)</td>\n",
       "      <td>(0.3297149122807017, 0.2415181489586251)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                   clr_M0_all_kmeans_5  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.4974415204678363, 0.4496015840162092)   \n",
       "\n",
       "                                                                   clr_M0_all_kmeans_6  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3682383040935673, 0.3513820496334686)   \n",
       "\n",
       "                                                                    clr_M0_all_kmeans_7  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3361111111111111, 0.40349933517825676)   \n",
       "\n",
       "                                                                   clr_M0_all_kmeans_8  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3441154970760234, 0.3258721698825865)   \n",
       "\n",
       "                                                                     clr_M1_all_kmeans_5  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                               NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.35164473684210523, 0.33349314628726395)   \n",
       "\n",
       "                                                                     clr_M1_all_kmeans_6  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                               NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.33903508771929824, 0.35536506960118075)   \n",
       "\n",
       "                                                                    clr_M1_all_kmeans_7  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.34755116959064325, 0.3363500677401237)   \n",
       "\n",
       "                                                                     clr_M1_all_kmeans_8  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                               NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.26593567251461986, 0.27121784188531944)   \n",
       "\n",
       "                                                                clr_guidi_all_kmeans_5  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.29780701754385963, 0.277132747426865)   \n",
       "\n",
       "                                                                 clr_guidi_all_kmeans_6  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.33464912280701753, 0.3137653253155704)   \n",
       "\n",
       "                                                              clr_guidi_all_kmeans_7  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                           NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.336951754385965, 0.328112584064965)   \n",
       "\n",
       "                                                                 clr_guidi_all_kmeans_8  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.28706140350877196, 0.2701996402517236)   \n",
       "\n",
       "                                                               clr_salazar_all_kmeans_5  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.4296052631578948, 0.40274108017980564)   \n",
       "\n",
       "                                                                clr_salazar_all_kmeans_6  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                               NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.44232456140350873, 0.39988779224890336)   \n",
       "\n",
       "                                                               clr_salazar_all_kmeans_7  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.33888888888888885, 0.3026676916557869)   \n",
       "\n",
       "                                                              clr_salazar_all_kmeans_8  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.354422514619883, 0.28794279601571265)   \n",
       "\n",
       "                                                               clr_stress_all_kmeans_5  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3914473684210526, 0.3113786463889663)   \n",
       "\n",
       "                                                               clr_stress_all_kmeans_6  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3901315789473684, 0.2715706070587606)   \n",
       "\n",
       "                                                                clr_stress_all_kmeans_7  \\\n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                              NaN   \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3600877192982456, 0.27114084324051646)   \n",
       "\n",
       "                                                               clr_stress_all_kmeans_8  \n",
       "matrix_tara_chile_adj_grids_25_all.tsv                                             NaN  \n",
       "matrix_tara_chile_adj_grids_25_all.tsv_s8_r9  (0.3297149122807017, 0.2415181489586251)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to show that the best column to try to predict is `clr_M0_all_kmeans_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'labels'\n",
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    #Cross-val to get optimal n_estimators\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        print(cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(f\"AUC Score (Train): {roc_auc_score(dtrain[target], dtrain_predprob, multi_class = 'ovo')}\")\n",
    "    xgb.plot_importance(alg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    booster = 'gbtree',\n",
    "                                    eval_metric='merror',\n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi:softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=10000,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "modelfit(model_1, train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,20,1),\n",
    " 'min_child_weight':range(1,30,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    "                                    param_grid = param_test1, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch1.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/100.0 for i in range(0,50)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test3, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch3.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(3,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(3,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test4, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch4.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(40,60,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07\n",
    "                                    ), \n",
    " param_grid = param_test5, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch5.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.2,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.55\n",
    "                                    ), \n",
    " param_grid = param_test6, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch6.fit(pred_train,lbs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'learning_rate':[0.1, 0.01,0.001,0.0001]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    ), \n",
    " param_grid = param_test7, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch7.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch7.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eso nos deja con el siguiente estimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.1,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(test[preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtest_predprob = final_model.predict_proba(test[preds])\n",
    "roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el modelo\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [5,8,10,12,15],\n",
    "    'subsample': [0.3, 0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[5*1e-2, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tiempo: 49 min\n",
    "\n",
    "Mejores hiperparámetros: `{'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'reg_alpha': 1e-05, 'subsample': 0.8}`.\n",
    "\n",
    "Mejor puntaje (accuracy): 0.7822656342513322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid2 = {\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'n_estimators': [5, 10 , 15],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search2 = GridSearchCV(estimator=model, param_grid=param_grid2, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search2.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search2.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores hiperparámetros: `{'colsample_bytree': 0.6, 'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 5, 'reg_alpha': 1e-05, 'subsample': 0.8}`\n",
    "\n",
    "Mejor puntaje: 0.7853041164911875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third and last refinement\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.005,  0.006, 0.004],\n",
    "    'n_estimators': [5,6, 7, 8],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.65, 0.55, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'min_child_weight': range(10)\n",
    "}\n",
    "\n",
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.005,  max_depth= 3, n_estimators= 8, reg_alpha= 1e-05, subsample= 0.75)\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=better_model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.1)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7 )\n",
    "better_model.fit(pred_train,lbs_train)\n",
    "xgb.plot_importance(better_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_report(model, test):\n",
    "    y_pred = model.predict(test[preds])\n",
    "    dtest_predprob = better_model.predict_proba(test[preds])\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')}\")\n",
    "    print(f\"f1 score: {f1_score(y_pred,test[target],average='macro')}\")\n",
    "    print(f\"Acc score: {accuracy_score(y_pred,test[target])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "naive_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(naive_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_imp(xgb):\n",
    "    imp_vals = xgb.get_booster().get_fscore()\n",
    "    feats_imp = pd.DataFrame(imp_vals,index=np.arange(2)).T\n",
    "    feats_imp.iloc[:,0]= feats_imp.index    \n",
    "    feats_imp.columns=['feature','importance']\n",
    "    feats_imp.sort_values('importance',inplace=True,ascending=False)\n",
    "    #feats_imp.reset_index(drop=True,inplace=True)\n",
    "    return feats_imp.set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_imps = get_xgb_imp(naive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = list(naive_imps[naive_imps['importance']>= 100].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7)\n",
    "better_model.fit(train.loc[:,important_feats],lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = better_model.predict(test[important_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test[target],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred,test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "cl_path = '../03_results/out_genomic_clusters/kmeans_results_ch.tsv'\n",
    "cl_df = pd.read_csv(cl_path, sep = \"\\t\")\n",
    "\n",
    "#Export to get datased to plot in 3D\n",
    "cols_to_get = cl_df.columns.to_list() + ['lat_cast','lon_cast', 'Depth [m]']\n",
    "file = pd.merge(md_df, cl_df, on='Samples')[cols_to_get]\n",
    "file.to_csv(path_or_buf='../03_results/clusters_with_coords.tsv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "cl_df.set_index('Samples', inplace=True)\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df = md_df[['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]', 'Depth level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Depth level\n",
       "EPI    88\n",
       "MES    41\n",
       "SRF    30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df['Depth level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for the main metadata feats ordered from smaller to larger value.\n",
    "xvals = {'all': 148, 'SRF': 27, 'MES': 38, 'EPI': 81}\n",
    "spaces = {'Temperature [ºC]':5,'Oxygen [ml/l]':7,'nitrates [uM]':5}\n",
    "for col in md_df.columns:\n",
    "    if col != 'Depth level':\n",
    "        space = spaces[col]\n",
    "        xval = xvals['all']\n",
    "        data = md_df[col].sort_values()\n",
    "        fig, ax = plt.subplots(figsize = (25,15))\n",
    "        ax.set_title(f'{col} values in TARA Chile (all)', fontsize = 35, pad = 20)\n",
    "        ax.set_xticks([])\n",
    "        plt.yticks(fontsize = 15)\n",
    "        plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "        plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "        ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "        ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "        ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "        plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "        plt.scatter(data.index,data)\n",
    "        sns.set(style='darkgrid')\n",
    "        path = f'../03_results/out_ch_data_analysis/{col[:-space]}_all'\n",
    "        plt.savefig(path)\n",
    "        for depth in md_df['Depth level'].unique():\n",
    "            #plot by depth level\n",
    "            xval = xvals[depth]\n",
    "            data = (md_df[md_df['Depth level'] == depth])[col].sort_values()\n",
    "            fig, ax = plt.subplots(figsize = (25,15))\n",
    "            ax.set_title(f'{col} values in TARA Chile ({depth})', fontsize = 35, pad = 20)\n",
    "            ax.set_xticks([])\n",
    "            plt.yticks(fontsize = 15)\n",
    "            plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "            plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "            ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "            ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "            ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "            plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "            plt.scatter(data.index,data)\n",
    "            sns.set(style='darkgrid')\n",
    "            path = f'../03_results/out_ch_data_analysis/{col[:-4]}_{depth}'\n",
    "            plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature_3_all: Temperature_3_all\n",
      "1    53\n",
      "0    53\n",
      "2    53\n",
      "Name: count, dtype: int64\n",
      "Oxygen_3_all: Oxygen_3_all\n",
      "2    53\n",
      "1    53\n",
      "0    53\n",
      "Name: count, dtype: int64\n",
      "nitrates_3_all: nitrates_3_all\n",
      "0    53\n",
      "1    53\n",
      "2    53\n",
      "Name: count, dtype: int64\n",
      "Temperature_3_SRF: Temperature_3_SRF\n",
      "0.0    10\n",
      "1.0    10\n",
      "2.0    10\n",
      "Name: count, dtype: int64\n",
      "Oxygen_3_SRF: Oxygen_3_SRF\n",
      "2.0    10\n",
      "1.0    10\n",
      "0.0    10\n",
      "Name: count, dtype: int64\n",
      "nitrates_3_SRF: nitrates_3_SRF\n",
      "2.0    10\n",
      "1.0    10\n",
      "0.0    10\n",
      "Name: count, dtype: int64\n",
      "Temperature_3_EPI: Temperature_3_EPI\n",
      "0.0    30\n",
      "2.0    30\n",
      "1.0    28\n",
      "Name: count, dtype: int64\n",
      "Oxygen_3_EPI: Oxygen_3_EPI\n",
      "2.0    30\n",
      "1.0    29\n",
      "0.0    29\n",
      "Name: count, dtype: int64\n",
      "nitrates_3_EPI: nitrates_3_EPI\n",
      "0.0    30\n",
      "1.0    29\n",
      "2.0    29\n",
      "Name: count, dtype: int64\n",
      "Temperature_3_MES: Temperature_3_MES\n",
      "0.0    14\n",
      "2.0    14\n",
      "1.0    13\n",
      "Name: count, dtype: int64\n",
      "Oxygen_3_MES: Oxygen_3_MES\n",
      "2.0    14\n",
      "0.0    14\n",
      "1.0    13\n",
      "Name: count, dtype: int64\n",
      "nitrates_3_MES: nitrates_3_MES\n",
      "0.0    14\n",
      "2.0    14\n",
      "1.0    13\n",
      "Name: count, dtype: int64\n",
      "Temperature_4_all: Temperature_4_all\n",
      "1    40\n",
      "0    40\n",
      "3    40\n",
      "2    39\n",
      "Name: count, dtype: int64\n",
      "Oxygen_4_all: Oxygen_4_all\n",
      "3    40\n",
      "1    40\n",
      "0    40\n",
      "2    39\n",
      "Name: count, dtype: int64\n",
      "nitrates_4_all: nitrates_4_all\n",
      "1    40\n",
      "0    40\n",
      "3    40\n",
      "2    39\n",
      "Name: count, dtype: int64\n",
      "Temperature_4_SRF: Temperature_4_SRF\n",
      "0.0    8\n",
      "3.0    8\n",
      "1.0    7\n",
      "2.0    7\n",
      "Name: count, dtype: int64\n",
      "Oxygen_4_SRF: Oxygen_4_SRF\n",
      "3.0    8\n",
      "0.0    8\n",
      "2.0    7\n",
      "1.0    7\n",
      "Name: count, dtype: int64\n",
      "nitrates_4_SRF: nitrates_4_SRF\n",
      "3.0    8\n",
      "0.0    8\n",
      "2.0    7\n",
      "1.0    7\n",
      "Name: count, dtype: int64\n",
      "Temperature_4_EPI: Temperature_4_EPI\n",
      "0.0    22\n",
      "1.0    22\n",
      "2.0    22\n",
      "3.0    22\n",
      "Name: count, dtype: int64\n",
      "Oxygen_4_EPI: Oxygen_4_EPI\n",
      "3.0    22\n",
      "2.0    22\n",
      "1.0    22\n",
      "0.0    22\n",
      "Name: count, dtype: int64\n",
      "nitrates_4_EPI: nitrates_4_EPI\n",
      "0.0    22\n",
      "1.0    22\n",
      "2.0    22\n",
      "3.0    22\n",
      "Name: count, dtype: int64\n",
      "Temperature_4_MES: Temperature_4_MES\n",
      "0.0    11\n",
      "2.0    10\n",
      "1.0    10\n",
      "3.0    10\n",
      "Name: count, dtype: int64\n",
      "Oxygen_4_MES: Oxygen_4_MES\n",
      "0.0    11\n",
      "3.0    10\n",
      "2.0    10\n",
      "1.0    10\n",
      "Name: count, dtype: int64\n",
      "nitrates_4_MES: nitrates_4_MES\n",
      "0.0    11\n",
      "1.0    10\n",
      "2.0    10\n",
      "3.0    10\n",
      "Name: count, dtype: int64\n",
      "Temperature_5_all: Temperature_5_all\n",
      "1    32\n",
      "3    32\n",
      "0    32\n",
      "4    32\n",
      "2    31\n",
      "Name: count, dtype: int64\n",
      "Oxygen_5_all: Oxygen_5_all\n",
      "4    32\n",
      "3    32\n",
      "1    32\n",
      "0    32\n",
      "2    31\n",
      "Name: count, dtype: int64\n",
      "nitrates_5_all: nitrates_5_all\n",
      "1    32\n",
      "0    32\n",
      "3    32\n",
      "4    32\n",
      "2    31\n",
      "Name: count, dtype: int64\n",
      "Temperature_5_SRF: Temperature_5_SRF\n",
      "0.0    6\n",
      "1.0    6\n",
      "2.0    6\n",
      "3.0    6\n",
      "4.0    6\n",
      "Name: count, dtype: int64\n",
      "Oxygen_5_SRF: Oxygen_5_SRF\n",
      "4.0    6\n",
      "2.0    6\n",
      "3.0    6\n",
      "1.0    6\n",
      "0.0    6\n",
      "Name: count, dtype: int64\n",
      "nitrates_5_SRF: nitrates_5_SRF\n",
      "3.0    6\n",
      "2.0    6\n",
      "1.0    6\n",
      "4.0    6\n",
      "0.0    6\n",
      "Name: count, dtype: int64\n",
      "Temperature_5_EPI: Temperature_5_EPI\n",
      "0.0    18\n",
      "4.0    18\n",
      "2.0    18\n",
      "1.0    17\n",
      "3.0    17\n",
      "Name: count, dtype: int64\n",
      "Oxygen_5_EPI: Oxygen_5_EPI\n",
      "4.0    18\n",
      "2.0    18\n",
      "0.0    18\n",
      "3.0    17\n",
      "1.0    17\n",
      "Name: count, dtype: int64\n",
      "nitrates_5_EPI: nitrates_5_EPI\n",
      "0.0    18\n",
      "2.0    18\n",
      "4.0    18\n",
      "1.0    17\n",
      "3.0    17\n",
      "Name: count, dtype: int64\n",
      "Temperature_5_MES: Temperature_5_MES\n",
      "0.0    9\n",
      "2.0    8\n",
      "3.0    8\n",
      "1.0    8\n",
      "4.0    8\n",
      "Name: count, dtype: int64\n",
      "Oxygen_5_MES: Oxygen_5_MES\n",
      "0.0    10\n",
      "4.0     8\n",
      "2.0     8\n",
      "3.0     8\n",
      "1.0     7\n",
      "Name: count, dtype: int64\n",
      "nitrates_5_MES: nitrates_5_MES\n",
      "0.0    9\n",
      "2.0    8\n",
      "1.0    8\n",
      "3.0    8\n",
      "4.0    8\n",
      "Name: count, dtype: int64\n",
      "Temperature_6_all: Temperature_6_all\n",
      "2    27\n",
      "0    27\n",
      "5    27\n",
      "1    26\n",
      "3    26\n",
      "4    26\n",
      "Name: count, dtype: int64\n",
      "Oxygen_6_all: Oxygen_6_all\n",
      "5    27\n",
      "2    27\n",
      "0    27\n",
      "3    26\n",
      "4    26\n",
      "1    26\n",
      "Name: count, dtype: int64\n",
      "nitrates_6_all: nitrates_6_all\n",
      "0    27\n",
      "2    27\n",
      "5    27\n",
      "1    26\n",
      "3    26\n",
      "4    26\n",
      "Name: count, dtype: int64\n",
      "Temperature_6_SRF: Temperature_6_SRF\n",
      "0.0    5\n",
      "1.0    5\n",
      "2.0    5\n",
      "3.0    5\n",
      "5.0    5\n",
      "4.0    5\n",
      "Name: count, dtype: int64\n",
      "Oxygen_6_SRF: Oxygen_6_SRF\n",
      "5.0    5\n",
      "3.0    5\n",
      "4.0    5\n",
      "1.0    5\n",
      "0.0    5\n",
      "2.0    5\n",
      "Name: count, dtype: int64\n",
      "nitrates_6_SRF: nitrates_6_SRF\n",
      "4.0    5\n",
      "3.0    5\n",
      "2.0    5\n",
      "1.0    5\n",
      "5.0    5\n",
      "0.0    5\n",
      "Name: count, dtype: int64\n",
      "Temperature_6_EPI: Temperature_6_EPI\n",
      "1.0    15\n",
      "0.0    15\n",
      "5.0    15\n",
      "4.0    15\n",
      "3.0    14\n",
      "2.0    14\n",
      "Name: count, dtype: int64\n",
      "Oxygen_6_EPI: Oxygen_6_EPI\n",
      "5.0    15\n",
      "4.0    15\n",
      "0.0    15\n",
      "2.0    15\n",
      "3.0    14\n",
      "1.0    14\n",
      "Name: count, dtype: int64\n",
      "nitrates_6_EPI: nitrates_6_EPI\n",
      "0.0    15\n",
      "1.0    15\n",
      "5.0    15\n",
      "3.0    15\n",
      "2.0    14\n",
      "4.0    14\n",
      "Name: count, dtype: int64\n",
      "Temperature_6_MES: Temperature_6_MES\n",
      "0.0    7\n",
      "1.0    7\n",
      "4.0    7\n",
      "2.0    7\n",
      "5.0    7\n",
      "3.0    6\n",
      "Name: count, dtype: int64\n",
      "Oxygen_6_MES: Oxygen_6_MES\n",
      "5.0    7\n",
      "4.0    7\n",
      "0.0    7\n",
      "1.0    7\n",
      "2.0    7\n",
      "3.0    6\n",
      "Name: count, dtype: int64\n",
      "nitrates_6_MES: nitrates_6_MES\n",
      "0.0    7\n",
      "2.0    7\n",
      "1.0    7\n",
      "4.0    7\n",
      "5.0    7\n",
      "3.0    6\n",
      "Name: count, dtype: int64\n",
      "Temperature_7_all: Temperature_7_all\n",
      "3    23\n",
      "1    23\n",
      "0    23\n",
      "6    23\n",
      "5    23\n",
      "2    22\n",
      "4    22\n",
      "Name: count, dtype: int64\n",
      "Oxygen_7_all: Oxygen_7_all\n",
      "6    23\n",
      "5    23\n",
      "3    23\n",
      "0    23\n",
      "1    23\n",
      "4    22\n",
      "2    22\n",
      "Name: count, dtype: int64\n",
      "nitrates_7_all: nitrates_7_all\n",
      "0    23\n",
      "1    23\n",
      "3    23\n",
      "6    23\n",
      "5    23\n",
      "2    22\n",
      "4    22\n",
      "Name: count, dtype: int64\n",
      "Temperature_7_SRF: Temperature_7_SRF\n",
      "0.0    5\n",
      "6.0    5\n",
      "1.0    4\n",
      "3.0    4\n",
      "4.0    4\n",
      "2.0    4\n",
      "5.0    4\n",
      "Name: count, dtype: int64\n",
      "Oxygen_7_SRF: Oxygen_7_SRF\n",
      "6.0    5\n",
      "0.0    5\n",
      "3.0    4\n",
      "5.0    4\n",
      "1.0    4\n",
      "4.0    4\n",
      "2.0    4\n",
      "Name: count, dtype: int64\n",
      "nitrates_7_SRF: nitrates_7_SRF\n",
      "6.0    5\n",
      "0.0    5\n",
      "5.0    4\n",
      "1.0    4\n",
      "3.0    4\n",
      "4.0    4\n",
      "2.0    4\n",
      "Name: count, dtype: int64\n",
      "Temperature_7_EPI: Temperature_7_EPI\n",
      "0.0    13\n",
      "4.0    13\n",
      "2.0    13\n",
      "6.0    13\n",
      "1.0    12\n",
      "3.0    12\n",
      "5.0    12\n",
      "Name: count, dtype: int64\n",
      "Oxygen_7_EPI: Oxygen_7_EPI\n",
      "6.0    13\n",
      "4.0    13\n",
      "2.0    13\n",
      "0.0    13\n",
      "5.0    12\n",
      "3.0    12\n",
      "1.0    12\n",
      "Name: count, dtype: int64\n",
      "nitrates_7_EPI: nitrates_7_EPI\n",
      "0.0    13\n",
      "2.0    13\n",
      "4.0    13\n",
      "6.0    13\n",
      "3.0    12\n",
      "1.0    12\n",
      "5.0    12\n",
      "Name: count, dtype: int64\n",
      "Temperature_7_MES: Temperature_7_MES\n",
      "0.0    6\n",
      "1.0    6\n",
      "4.0    6\n",
      "2.0    6\n",
      "5.0    6\n",
      "6.0    6\n",
      "3.0    5\n",
      "Name: count, dtype: int64\n",
      "Oxygen_7_MES: Oxygen_7_MES\n",
      "6.0    6\n",
      "4.0    6\n",
      "5.0    6\n",
      "2.0    6\n",
      "1.0    6\n",
      "0.0    6\n",
      "3.0    5\n",
      "Name: count, dtype: int64\n",
      "nitrates_7_MES: nitrates_7_MES\n",
      "0.0    6\n",
      "1.0    6\n",
      "2.0    6\n",
      "6.0    6\n",
      "4.0    6\n",
      "5.0    6\n",
      "3.0    5\n",
      "Name: count, dtype: int64\n",
      "Temperature_8_all: Temperature_8_all\n",
      "3    20\n",
      "2    20\n",
      "1    20\n",
      "0    20\n",
      "5    20\n",
      "6    20\n",
      "7    20\n",
      "4    19\n",
      "Name: count, dtype: int64\n",
      "Oxygen_8_all: Oxygen_8_all\n",
      "7    20\n",
      "5    20\n",
      "6    20\n",
      "3    20\n",
      "1    20\n",
      "2    20\n",
      "0    20\n",
      "4    19\n",
      "Name: count, dtype: int64\n",
      "nitrates_8_all: nitrates_8_all\n",
      "2    20\n",
      "0    20\n",
      "1    20\n",
      "3    20\n",
      "5    20\n",
      "6    20\n",
      "7    20\n",
      "4    19\n",
      "Name: count, dtype: int64\n",
      "Temperature_8_SRF: Temperature_8_SRF\n",
      "0.0    4\n",
      "1.0    4\n",
      "3.0    4\n",
      "4.0    4\n",
      "7.0    4\n",
      "6.0    4\n",
      "5.0    3\n",
      "2.0    3\n",
      "Name: count, dtype: int64\n",
      "Oxygen_8_SRF: Oxygen_8_SRF\n",
      "7.0    4\n",
      "6.0    4\n",
      "4.0    4\n",
      "1.0    4\n",
      "0.0    4\n",
      "3.0    4\n",
      "5.0    3\n",
      "2.0    3\n",
      "Name: count, dtype: int64\n",
      "nitrates_8_SRF: nitrates_8_SRF\n",
      "6.0    4\n",
      "4.0    4\n",
      "3.0    4\n",
      "1.0    4\n",
      "0.0    4\n",
      "7.0    4\n",
      "2.0    3\n",
      "5.0    3\n",
      "Name: count, dtype: int64\n",
      "Temperature_8_EPI: Temperature_8_EPI\n",
      "1.0    11\n",
      "0.0    11\n",
      "3.0    11\n",
      "2.0    11\n",
      "4.0    11\n",
      "5.0    11\n",
      "7.0    11\n",
      "6.0    11\n",
      "Name: count, dtype: int64\n",
      "Oxygen_8_EPI: Oxygen_8_EPI\n",
      "7.0    11\n",
      "4.0    11\n",
      "5.0    11\n",
      "6.0    11\n",
      "3.0    11\n",
      "2.0    11\n",
      "1.0    11\n",
      "0.0    11\n",
      "Name: count, dtype: int64\n",
      "nitrates_8_EPI: nitrates_8_EPI\n",
      "0.0    11\n",
      "2.0    11\n",
      "3.0    11\n",
      "1.0    11\n",
      "4.0    11\n",
      "5.0    11\n",
      "6.0    11\n",
      "7.0    11\n",
      "Name: count, dtype: int64\n",
      "Temperature_8_MES: Temperature_8_MES\n",
      "0.0    6\n",
      "4.0    5\n",
      "1.0    5\n",
      "3.0    5\n",
      "5.0    5\n",
      "6.0    5\n",
      "2.0    5\n",
      "7.0    5\n",
      "Name: count, dtype: int64\n",
      "Oxygen_8_MES: Oxygen_8_MES\n",
      "0.0    6\n",
      "7.0    5\n",
      "5.0    5\n",
      "4.0    5\n",
      "2.0    5\n",
      "6.0    5\n",
      "3.0    5\n",
      "1.0    5\n",
      "Name: count, dtype: int64\n",
      "nitrates_8_MES: nitrates_8_MES\n",
      "0.0    6\n",
      "1.0    5\n",
      "3.0    5\n",
      "2.0    5\n",
      "5.0    5\n",
      "4.0    5\n",
      "6.0    5\n",
      "7.0    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Metadata quantile-based binning\n",
    "n_bins = [3,4,5,6,7,8] \n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all','SRF','EPI','MES']\n",
    "for n in n_bins:\n",
    "    q = 1/n\n",
    "    for layer in layers:\n",
    "        if layer == 'all':\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[feat] \n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df[binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")\n",
    "        else:\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[md_df['Depth level'] == layer][feat]\n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df.loc[md_df['Depth level'] == layer,binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature [ºC]</th>\n",
       "      <th>Oxygen [ml/l]</th>\n",
       "      <th>nitrates [uM]</th>\n",
       "      <th>Depth level</th>\n",
       "      <th>Temperature_3_all</th>\n",
       "      <th>Oxygen_3_all</th>\n",
       "      <th>nitrates_3_all</th>\n",
       "      <th>Temperature_3_SRF</th>\n",
       "      <th>Oxygen_3_SRF</th>\n",
       "      <th>nitrates_3_SRF</th>\n",
       "      <th>...</th>\n",
       "      <th>nitrates_8_all</th>\n",
       "      <th>Temperature_8_SRF</th>\n",
       "      <th>Oxygen_8_SRF</th>\n",
       "      <th>nitrates_8_SRF</th>\n",
       "      <th>Temperature_8_EPI</th>\n",
       "      <th>Oxygen_8_EPI</th>\n",
       "      <th>nitrates_8_EPI</th>\n",
       "      <th>Temperature_8_MES</th>\n",
       "      <th>Oxygen_8_MES</th>\n",
       "      <th>nitrates_8_MES</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Samples</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>S01_Z00</th>\n",
       "      <td>11.272</td>\n",
       "      <td>6.665</td>\n",
       "      <td>12.3675</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S01_Z01</th>\n",
       "      <td>10.569</td>\n",
       "      <td>5.856</td>\n",
       "      <td>2.0850</td>\n",
       "      <td>EPI</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S01_Z05</th>\n",
       "      <td>9.060</td>\n",
       "      <td>4.836</td>\n",
       "      <td>13.6680</td>\n",
       "      <td>EPI</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S02_Z00</th>\n",
       "      <td>11.465</td>\n",
       "      <td>6.430</td>\n",
       "      <td>6.7840</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S02_Z01</th>\n",
       "      <td>11.426</td>\n",
       "      <td>6.413</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>EPI</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S34_Z01</th>\n",
       "      <td>15.248</td>\n",
       "      <td>0.563</td>\n",
       "      <td>17.7125</td>\n",
       "      <td>EPI</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S34_Z03</th>\n",
       "      <td>13.586</td>\n",
       "      <td>0.069</td>\n",
       "      <td>8.5005</td>\n",
       "      <td>EPI</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S34_Z05</th>\n",
       "      <td>13.168</td>\n",
       "      <td>0.057</td>\n",
       "      <td>11.6500</td>\n",
       "      <td>EPI</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S34_Z07</th>\n",
       "      <td>12.868</td>\n",
       "      <td>0.047</td>\n",
       "      <td>14.5325</td>\n",
       "      <td>MES</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S34_Z09</th>\n",
       "      <td>9.750</td>\n",
       "      <td>0.036</td>\n",
       "      <td>31.8225</td>\n",
       "      <td>MES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Temperature [ºC]  Oxygen [ml/l]  nitrates [uM] Depth level  \\\n",
       "Samples                                                               \n",
       "S01_Z00            11.272          6.665        12.3675         SRF   \n",
       "S01_Z01            10.569          5.856         2.0850         EPI   \n",
       "S01_Z05             9.060          4.836        13.6680         EPI   \n",
       "S02_Z00            11.465          6.430         6.7840         SRF   \n",
       "S02_Z01            11.426          6.413         0.2675         EPI   \n",
       "...                   ...            ...            ...         ...   \n",
       "S34_Z01            15.248          0.563        17.7125         EPI   \n",
       "S34_Z03            13.586          0.069         8.5005         EPI   \n",
       "S34_Z05            13.168          0.057        11.6500         EPI   \n",
       "S34_Z07            12.868          0.047        14.5325         MES   \n",
       "S34_Z09             9.750          0.036        31.8225         MES   \n",
       "\n",
       "         Temperature_3_all  Oxygen_3_all  nitrates_3_all  Temperature_3_SRF  \\\n",
       "Samples                                                                       \n",
       "S01_Z00                  1             2               0                0.0   \n",
       "S01_Z01                  0             2               0                NaN   \n",
       "S01_Z05                  0             1               1                NaN   \n",
       "S02_Z00                  1             2               0                0.0   \n",
       "S02_Z01                  1             2               0                NaN   \n",
       "...                    ...           ...             ...                ...   \n",
       "S34_Z01                  2             0               1                NaN   \n",
       "S34_Z03                  2             0               0                NaN   \n",
       "S34_Z05                  2             0               0                NaN   \n",
       "S34_Z07                  2             0               1                NaN   \n",
       "S34_Z09                  0             0               2                NaN   \n",
       "\n",
       "         Oxygen_3_SRF  nitrates_3_SRF  ...  nitrates_8_all  Temperature_8_SRF  \\\n",
       "Samples                                ...                                      \n",
       "S01_Z00           2.0             2.0  ...               2                0.0   \n",
       "S01_Z01           NaN             NaN  ...               0                NaN   \n",
       "S01_Z05           NaN             NaN  ...               2                NaN   \n",
       "S02_Z00           2.0             1.0  ...               1                0.0   \n",
       "S02_Z01           NaN             NaN  ...               0                NaN   \n",
       "...               ...             ...  ...             ...                ...   \n",
       "S34_Z01           NaN             NaN  ...               3                NaN   \n",
       "S34_Z03           NaN             NaN  ...               1                NaN   \n",
       "S34_Z05           NaN             NaN  ...               2                NaN   \n",
       "S34_Z07           NaN             NaN  ...               2                NaN   \n",
       "S34_Z09           NaN             NaN  ...               7                NaN   \n",
       "\n",
       "         Oxygen_8_SRF  nitrates_8_SRF  Temperature_8_EPI  Oxygen_8_EPI  \\\n",
       "Samples                                                                  \n",
       "S01_Z00           7.0             6.0                NaN           NaN   \n",
       "S01_Z01           NaN             NaN                1.0           7.0   \n",
       "S01_Z05           NaN             NaN                0.0           4.0   \n",
       "S02_Z00           6.0             4.0                NaN           NaN   \n",
       "S02_Z01           NaN             NaN                3.0           7.0   \n",
       "...               ...             ...                ...           ...   \n",
       "S34_Z01           NaN             NaN                7.0           0.0   \n",
       "S34_Z03           NaN             NaN                6.0           0.0   \n",
       "S34_Z05           NaN             NaN                5.0           0.0   \n",
       "S34_Z07           NaN             NaN                NaN           NaN   \n",
       "S34_Z09           NaN             NaN                NaN           NaN   \n",
       "\n",
       "         nitrates_8_EPI  Temperature_8_MES  Oxygen_8_MES  nitrates_8_MES  \n",
       "Samples                                                                   \n",
       "S01_Z00             NaN                NaN           NaN             NaN  \n",
       "S01_Z01             0.0                NaN           NaN             NaN  \n",
       "S01_Z05             2.0                NaN           NaN             NaN  \n",
       "S02_Z00             NaN                NaN           NaN             NaN  \n",
       "S02_Z01             0.0                NaN           NaN             NaN  \n",
       "...                 ...                ...           ...             ...  \n",
       "S34_Z01             4.0                NaN           NaN             NaN  \n",
       "S34_Z03             1.0                NaN           NaN             NaN  \n",
       "S34_Z05             1.0                NaN           NaN             NaN  \n",
       "S34_Z07             NaN                7.0           0.0             0.0  \n",
       "S34_Z09             NaN                4.0           0.0             5.0  \n",
       "\n",
       "[159 rows x 76 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df\n",
    "for layer in layers:\n",
    "    if layer == 'all':\n",
    "        data = md_df[[col for col in md_df.columns if layer in col]]\n",
    "    else:\n",
    "        data = md_df[md_df['Depth level'] == layer][[col for col in md_df.columns if layer in col]]\n",
    "    data.to_csv(path_or_buf= f'../03_results/metadata_based_clusters/metadata_clusters_{layer}.tsv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = '../03_results/metadata_based_clusters'\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all'\n",
    "#          ,'SRF','EPI','MES'\n",
    "          ]\n",
    "columns_to_use = []\n",
    "for feat in feats:\n",
    "    clean_feat = feat.split(\" \", 1)[0]\n",
    "    for n in desired_clusters:\n",
    "        columns_to_use.append(clean_feat+'_'+n)\n",
    "\n",
    "results_df = pd.DataFrame(index=layers, columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for target_vars_filename in [f for f in os.listdir(cluster_dir) if not f.split('_')[-1] == 'metrics.tsv']:\n",
    "    target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "    target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    layer = target_vars_filename[-7:-4]\n",
    "    for col in columns_to_use:\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        feat = col.split('_')[0]\n",
    "        target_column = f\"{feat}_{n_clusters}_{layer}\"\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[layer, col] = f\"({avg_accuracy}, {avg_f1_macro})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature_6</th>\n",
       "      <th>Temperature_5</th>\n",
       "      <th>Temperature_7</th>\n",
       "      <th>Temperature_8</th>\n",
       "      <th>Oxygen_6</th>\n",
       "      <th>Oxygen_5</th>\n",
       "      <th>Oxygen_7</th>\n",
       "      <th>Oxygen_8</th>\n",
       "      <th>nitrates_6</th>\n",
       "      <th>nitrates_5</th>\n",
       "      <th>nitrates_7</th>\n",
       "      <th>nitrates_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>(0.18819444444444444, 0.17157521619327173)</td>\n",
       "      <td>(0.21451023391812868, 0.1981890177723511)</td>\n",
       "      <td>(0.16191520467836257, 0.15150427174236697)</td>\n",
       "      <td>(0.15584795321637424, 0.13722993827160496)</td>\n",
       "      <td>(0.2799342105263158, 0.25637708484930705)</td>\n",
       "      <td>(0.33665935672514613, 0.31946140279473606)</td>\n",
       "      <td>(0.24883040935672515, 0.22163871181728323)</td>\n",
       "      <td>(0.23826754385964913, 0.2134975749559083)</td>\n",
       "      <td>(0.19119152046783625, 0.17727027345082902)</td>\n",
       "      <td>(0.20098684210526313, 0.1846725342558676)</td>\n",
       "      <td>(0.20171783625730993, 0.18254769921436587)</td>\n",
       "      <td>(0.16765350877192983, 0.14411237874779542)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EPI</th>\n",
       "      <td>(0.369949494949495, 0.32175925925925924)</td>\n",
       "      <td>(0.29545454545454547, 0.2662477954144621)</td>\n",
       "      <td>(0.23232323232323232, 0.19198318216175358)</td>\n",
       "      <td>(0.16287878787878787, 0.13215525793650792)</td>\n",
       "      <td>(0.25, 0.21746031746031746)</td>\n",
       "      <td>(0.2638888888888889, 0.23175925925925928)</td>\n",
       "      <td>(0.21969696969696967, 0.19366654069035022)</td>\n",
       "      <td>(0.19823232323232323, 0.1722883597883598)</td>\n",
       "      <td>(0.23989898989898992, 0.21592261904761903)</td>\n",
       "      <td>(0.18434343434343434, 0.16140652557319224)</td>\n",
       "      <td>(0.16414141414141414, 0.13385298563869993)</td>\n",
       "      <td>(0.19191919191919193, 0.1548735119047619)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MES</th>\n",
       "      <td>(0.275462962962963, 0.2122685185185185)</td>\n",
       "      <td>(0.16990740740740742, 0.11842592592592593)</td>\n",
       "      <td>(0.36904761904761896, 0.2959656084656085)</td>\n",
       "      <td>(0.3402777777777778, 0.26846064814814813)</td>\n",
       "      <td>(0.28009259259259256, 0.2138117283950617)</td>\n",
       "      <td>(0.24537037037037035, 0.1786111111111111)</td>\n",
       "      <td>(0.3134920634920635, 0.2397486772486772)</td>\n",
       "      <td>(0.4322916666666667, 0.34803240740740743)</td>\n",
       "      <td>(0.30092592592592593, 0.23750000000000002)</td>\n",
       "      <td>(0.20324074074074072, 0.14935185185185185)</td>\n",
       "      <td>(0.33333333333333326, 0.25641534391534393)</td>\n",
       "      <td>(0.4947916666666667, 0.40636574074074067)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SRF</th>\n",
       "      <td>(0.8263888888888888, 0.7812499999999999)</td>\n",
       "      <td>(0.7250000000000001, 0.6633333333333333)</td>\n",
       "      <td>(0.8789682539682538, 0.8428571428571427)</td>\n",
       "      <td>(0.8611111111111112, 0.8223379629629629)</td>\n",
       "      <td>(0.6296296296296295, 0.5575617283950617)</td>\n",
       "      <td>(0.6638888888888889, 0.5925925925925927)</td>\n",
       "      <td>(0.751984126984127, 0.6970238095238095)</td>\n",
       "      <td>(0.8663194444444444, 0.8234953703703702)</td>\n",
       "      <td>(0.6111111111111112, 0.5381172839506172)</td>\n",
       "      <td>(0.711111111111111, 0.6412037037037037)</td>\n",
       "      <td>(0.8095238095238094, 0.7539682539682538)</td>\n",
       "      <td>(0.8576388888888888, 0.8194444444444443)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Temperature_6  \\\n",
       "all  (0.18819444444444444, 0.17157521619327173)   \n",
       "EPI    (0.369949494949495, 0.32175925925925924)   \n",
       "MES     (0.275462962962963, 0.2122685185185185)   \n",
       "SRF    (0.8263888888888888, 0.7812499999999999)   \n",
       "\n",
       "                                  Temperature_5  \\\n",
       "all   (0.21451023391812868, 0.1981890177723511)   \n",
       "EPI   (0.29545454545454547, 0.2662477954144621)   \n",
       "MES  (0.16990740740740742, 0.11842592592592593)   \n",
       "SRF    (0.7250000000000001, 0.6633333333333333)   \n",
       "\n",
       "                                  Temperature_7  \\\n",
       "all  (0.16191520467836257, 0.15150427174236697)   \n",
       "EPI  (0.23232323232323232, 0.19198318216175358)   \n",
       "MES   (0.36904761904761896, 0.2959656084656085)   \n",
       "SRF    (0.8789682539682538, 0.8428571428571427)   \n",
       "\n",
       "                                  Temperature_8  \\\n",
       "all  (0.15584795321637424, 0.13722993827160496)   \n",
       "EPI  (0.16287878787878787, 0.13215525793650792)   \n",
       "MES   (0.3402777777777778, 0.26846064814814813)   \n",
       "SRF    (0.8611111111111112, 0.8223379629629629)   \n",
       "\n",
       "                                      Oxygen_6  \\\n",
       "all  (0.2799342105263158, 0.25637708484930705)   \n",
       "EPI                (0.25, 0.21746031746031746)   \n",
       "MES  (0.28009259259259256, 0.2138117283950617)   \n",
       "SRF   (0.6296296296296295, 0.5575617283950617)   \n",
       "\n",
       "                                       Oxygen_5  \\\n",
       "all  (0.33665935672514613, 0.31946140279473606)   \n",
       "EPI   (0.2638888888888889, 0.23175925925925928)   \n",
       "MES   (0.24537037037037035, 0.1786111111111111)   \n",
       "SRF    (0.6638888888888889, 0.5925925925925927)   \n",
       "\n",
       "                                       Oxygen_7  \\\n",
       "all  (0.24883040935672515, 0.22163871181728323)   \n",
       "EPI  (0.21969696969696967, 0.19366654069035022)   \n",
       "MES    (0.3134920634920635, 0.2397486772486772)   \n",
       "SRF     (0.751984126984127, 0.6970238095238095)   \n",
       "\n",
       "                                      Oxygen_8  \\\n",
       "all  (0.23826754385964913, 0.2134975749559083)   \n",
       "EPI  (0.19823232323232323, 0.1722883597883598)   \n",
       "MES  (0.4322916666666667, 0.34803240740740743)   \n",
       "SRF   (0.8663194444444444, 0.8234953703703702)   \n",
       "\n",
       "                                     nitrates_6  \\\n",
       "all  (0.19119152046783625, 0.17727027345082902)   \n",
       "EPI  (0.23989898989898992, 0.21592261904761903)   \n",
       "MES  (0.30092592592592593, 0.23750000000000002)   \n",
       "SRF    (0.6111111111111112, 0.5381172839506172)   \n",
       "\n",
       "                                     nitrates_5  \\\n",
       "all   (0.20098684210526313, 0.1846725342558676)   \n",
       "EPI  (0.18434343434343434, 0.16140652557319224)   \n",
       "MES  (0.20324074074074072, 0.14935185185185185)   \n",
       "SRF     (0.711111111111111, 0.6412037037037037)   \n",
       "\n",
       "                                     nitrates_7  \\\n",
       "all  (0.20171783625730993, 0.18254769921436587)   \n",
       "EPI  (0.16414141414141414, 0.13385298563869993)   \n",
       "MES  (0.33333333333333326, 0.25641534391534393)   \n",
       "SRF    (0.8095238095238094, 0.7539682539682538)   \n",
       "\n",
       "                                     nitrates_8  \n",
       "all  (0.16765350877192983, 0.14411237874779542)  \n",
       "EPI   (0.19191919191919193, 0.1548735119047619)  \n",
       "MES   (0.4947916666666667, 0.40636574074074067)  \n",
       "SRF    (0.8576388888888888, 0.8194444444444443)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(path_or_buf='../03_results/metadata_based_clusters/metadata_cluster_metrics_splitted.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(cluster_dir)[0][-7:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
