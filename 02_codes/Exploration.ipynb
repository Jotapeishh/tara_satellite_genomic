{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary exploration: reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this notebook is to study the data and compare what we get by calculating things and the corresponding values stored in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by picking one of the TARA matrices stored in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df1.columns and 'bbp_unc_443' in df1.columns:\n",
    "    df1 = df1.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df2.columns and 'bbp_unc_443' in df2.columns:\n",
    "    df2= df2.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1.columns == df2.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (df1.values - df2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err = diff.sum()\n",
    "print(total_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err/df1.values.shape[0]*df1.values.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too high, so lets dig into that error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = diff.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(idxs[0])):\n",
    "    i = idxs[0][k]\n",
    "    j = idxs[1][k]\n",
    "    print(f\" Difference at ({i},{j}) = {diff[i,j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is only one 'big' error in the last element, meanwhile the rest can be attributed to float point. The numerical coordinates of the attribute are (79,31) as we can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[79].keys()[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problematic sample is `TSC280`, and the problematic feature is `PAR.par`. Looking at the individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Repo-stored value: {df1['PAR.par'].loc['TSC280']}.\\nNew obtained value: {df2['PAR.par'].loc['TSC280']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to recalculate this value 'by hand' and compare it to the two that we have. This will be done with the source code of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def find_satellite_file(directory, pattern):\n",
    "    regex = re.compile(pattern)\n",
    "    for file in os.listdir(directory):\n",
    "        if regex.match(file):\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "def select_n_nearest_valid(ds, feature, latitude, longitude, n):\n",
    "    latitudes = ds['lat'].values\n",
    "    longitudes = ds['lon'].values\n",
    "    \n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes, indexing='ij')\n",
    "    distances = np.sqrt((lat_grid - latitude)**2 + (lon_grid - longitude)**2)\n",
    "    \n",
    "    sorted_indices = np.unravel_index(np.argsort(distances, axis=None), distances.shape)\n",
    "    \n",
    "    valid_points = []\n",
    "    for i in range(len(sorted_indices[0])):\n",
    "        lat_idx = sorted_indices[0][i]\n",
    "        lon_idx = sorted_indices[1][i]\n",
    "        data_point = ds[feature].isel(lat=lat_idx, lon=lon_idx)\n",
    "        if not np.isnan(data_point.values):\n",
    "            valid_points.append(data_point.values)\n",
    "        if len(valid_points) >= n:\n",
    "            break\n",
    "    \n",
    "    if len(valid_points) > 0:\n",
    "        return np.mean(valid_points)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25 #number of near points.\n",
    "file_path = '../01_data/01_biological_data'\n",
    "file_name = 'metadata.tsv'\n",
    "sat_data_path = '../01_data/00_satellite_data'\n",
    "\n",
    "output_dir = '../01_data/02_satellite_data_processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file = os.path.join(file_path, file_name)\n",
    "md = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "md_srf = md[md.Layer == 'SRF'].copy()\n",
    "md_srf['Event.date.YYYYMM'] = md_srf['Event.date'].str[:7].str.replace('-', '')\n",
    "md_srf['Event.date.YYYYMM01'] = md_srf['Event.date'].str[:7].str.replace('-', '')+'01'\n",
    "\n",
    "satellite_features = [\n",
    "    'CHL.chlor_a', 'FLH.nflh', 'FLH.ipar', 'IOP.adg_unc_443', 'IOP.adg_443',\n",
    "    'IOP.aph_unc_443', 'IOP.aph_44', 'IOP.bbp_s', 'IOP.adg_s', 'bbp_unc_443', \n",
    "    'IOP.bbp_443', 'IOP.a_412', 'IOP.a_443', 'IOP.a_469', 'IOP.a_488', 'IOP.a_531', \n",
    "    'IOP.a_547', 'IOP.a_555', 'IOP.a_645', 'IOP.a_667', 'IOP.a_678', 'IOP.bb_412', \n",
    "    'IOP.bb_443', 'IOP.bb_469', 'IOP.bb_488', 'IOP.bb_531', 'IOP.bb_547', 'IOP.bb_555', \n",
    "    'IOP.bb_645', 'IOP.bb_667', 'IOP.bb_678', 'KD.Kd_490', 'NSST.sst', 'PAR.par', \n",
    "    'PIC.pic', 'POC.poc', 'RRS.aot_869', 'RRS.angstrom', 'RRS.Rrs_412', 'RRS.Rrs_443', \n",
    "    'RRS.Rrs_469', 'RRS.Rrs_488', 'RRS.Rrs_531', 'RRS.Rrs_547', 'RRS.Rrs_555', \n",
    "    'RRS.Rrs_645', 'RRS.Rrs_667', 'RRS.Rrs_678', 'SST.sst'\n",
    "]\n",
    "\n",
    "satellite_data_terra = pd.DataFrame(index=md_srf.index, columns=satellite_features)\n",
    "satellite_data_aqua = pd.DataFrame(index=md_srf.index, columns=satellite_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = md_srf.loc['TSC280'] #We use the obtained key.\n",
    "feature = 'PAR.par' #We use the obtained feature.\n",
    "latitude = row['Latitude']\n",
    "longitude = row['Longitude']\n",
    "date = row['Event.date.YYYYMM']\n",
    "resolution = '9km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_terra = rf\"TERRA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_terra = find_satellite_file(sat_data_path, pattern_terra)\n",
    "pattern_aqua = rf\"AQUA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_aqua = find_satellite_file(sat_data_path, pattern_aqua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_terra = xr.open_dataset(file_path_terra)\n",
    "ds_aqua = xr.open_dataset(file_path_aqua)\n",
    "terra_values = []\n",
    "aqua_values = []\n",
    "try:\n",
    "    variable_name = feature.split('.')[1]\n",
    "    \n",
    "    data_point_terra = select_n_nearest_valid(ds_terra, variable_name, latitude, longitude, 25)\n",
    "    terra_values.append(data_point_terra)\n",
    "\n",
    "    data_point_aqua = select_n_nearest_valid(ds_aqua, variable_name, latitude, longitude, 25)\n",
    "    aqua_values.append(data_point_aqua)\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Feature {feature} not found in dataset\")\n",
    "finally:\n",
    "    ds_terra.close()\n",
    "    ds_aqua.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_arr = np.array(aqua_values)\n",
    "terra_arr = np.array(terra_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_arr = (aqua_arr + terra_arr)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Value by hand: {final_arr[0]}.\\nValue stored in repo: {df1['PAR.par'].loc['TSC280']}. \\nNew value obtained by script: {df2['PAR.par'].loc['TSC280']}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the value obtained by hand is closer to the '_new' value than to the one stored in the repo, with error likely of approximation. The difference of the old value and the new value is of order $10^{-3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us make a simmilar study to the `n=1` case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01.tsv\", sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df3.columns and 'bbp_unc_443' in df3.columns:\n",
    "    df3 = df3.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df4.columns and 'bbp_unc_443' in df4.columns:\n",
    "    df4= df4.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.columns == df4.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = (df3.values - df4.values)\n",
    "print(diff2.max())\n",
    "print(diff2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err2 = diff2.sum()\n",
    "print(total_err2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the values are all equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the impression that the issue is somewhere between the part where the closest sat reads (physicall distance-wise) are chosen. There is a pending plan of testing with a third run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate further one may plot the points selected in the proccess for both versions and check if they are the same or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be related to eventual changes to the original database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea: para mie. 6/nov\n",
    "Repetir todo el proceso de clustering con TARA Chile desde cero. De momento esto contempla:\n",
    "* Recuperar el `metadata_chile.tsv`.\n",
    "* Re-escribir el script `00_00_extract_satellite_data.py` porque cambian varias keys.\n",
    "* Unificar formato de fecha en el `metadata_chile.tsv`.\n",
    "\n",
    "## Cosas tangenciales a las que ponerle tiempo si es posible.\n",
    "* Estandarizar una pipeline que tome las keys como inputs.\n",
    "* Documentar los módulos y funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster projection on map\n",
    "We need to study the behavior of the drawing code on the new data, since it has different format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = md.join(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_on_map(merged_data, cluster_column):\n",
    "    filtered_data = merged_data[~merged_data[cluster_column].isna()] # filter out rows where cluster_column is NaN\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = axes.flatten()  # flatten the array of axes for easy iteration\n",
    "    \n",
    "    plot_titles = [\n",
    "        f'Clusters Projection: {cluster_column}',\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    data_columns = [\n",
    "        cluster_column,\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    \n",
    "    unique_clusters = filtered_data[cluster_column].unique()\n",
    "    num_clusters = len(unique_clusters)\n",
    "    marker_styles = ['o', 's', '^', 'v', '<', '>', 'd', 'p', 'h', 'H', '*', 'x', '+', 'D']\n",
    "    if num_clusters > len(marker_styles):\n",
    "        marker_styles = (marker_styles * ((num_clusters // len(marker_styles)) + 1))[:num_clusters]\n",
    "    cluster_marker_map = dict(zip(unique_clusters, marker_styles))\n",
    "    \n",
    "    env_vars = ['Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']\n",
    "    norms = {}\n",
    "    \n",
    "    for data_column in env_vars:\n",
    "        vmin = filtered_data[data_column].min()\n",
    "        vmax = filtered_data[data_column].max()\n",
    "        norms[data_column] = Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.set_extent([-80, -67, -55,-17])\n",
    "\n",
    "        ax.add_feature(cfeature.LAND)\n",
    "        ax.add_feature(cfeature.OCEAN)\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        \n",
    "        ax.set_title(plot_titles[idx])\n",
    "        \n",
    "        data_column = data_columns[idx]\n",
    "        plot_data = filtered_data[~filtered_data[data_column].isna()]\n",
    "        \n",
    "        if idx == 0:\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    label=f'Cluster {cluster_id}',\n",
    "                    s=35,\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            ax.legend(loc='upper left')\n",
    "        else:\n",
    "            norm = norms[data_column]\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                sc = ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    c=cluster_points[data_column],\n",
    "                    s=35,\n",
    "                    cmap='viridis',\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    edgecolors='black',\n",
    "                    norm=norm,\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.5)\n",
    "            cbar.set_label(data_column)\n",
    "            handles = []\n",
    "            for cluster_id in unique_clusters:\n",
    "                marker = cluster_marker_map[cluster_id]\n",
    "                handle = plt.Line2D([], [], color='black', marker=marker, linestyle='', markersize=8, label=f'Cluster {cluster_id}')\n",
    "                handles.append(handle)\n",
    "            ax.legend(handles=handles, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'clusters_{cluster_column}.pdf')\n",
    "    plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mañana: doble plan de acción.\n",
    "* Tomar los clusters viejos y de ahí filtrar y mapear. ('Más fácil capaz') (esto ya está básicamente hechp)\n",
    "* Filtrar, clusterizar, y mapear. (Aprovecha que se quita una 'variable' de la clusterización):\n",
    "    * En vez de leer las matrices de clusters, leer las matrices de data bio\n",
    "* Comparar como lucen\n",
    "\n",
    "La nomenclatura será, en el mismo orden:\n",
    "* `_cf`: clustered filtered.\n",
    "* `_fc`: filtered clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Cluster -> Filtro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation.\n",
    "This code right ahead takes all the already clustered data, and saves a file of the selection of those samples that are 'SRF'. If a file named `kmeans_results_ch_srf_clustered_filtered.tsv` exists already in the folder, there is no need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch.tsv'\n",
    "\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in matrix_files:\n",
    "    pth = f\"{input_dir}/{file}\"\n",
    "    #bio_mtrx es en verdad cluster_mtrx. Arreglar para legibilidad.\n",
    "    clstr_mtrx =  pd.read_csv(file_path, sep='\\t', index_col=0) \n",
    "    meta_mtrx = pd.read_csv('../01_data/01_biological_data/metadata_chile.tsv', sep='\\t', index_col=0) \n",
    "    meta_mtrx = meta_mtrx[meta_mtrx['Depth level']== 'SRF']\n",
    "    dirty_df = meta_mtrx.join(clstr_mtrx)\n",
    "    clean_df = dirty_df.drop(meta_mtrx.columns, axis=1)\n",
    "    new_keys = {col: col+'_cf' for col in clean_df.columns}\n",
    "    clean_df.rename(columns = new_keys, inplace=True)\n",
    "    output_filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "    clean_df.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Filtro -> Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "We start by collecting the bio data, filtering only the SRF samples, and saving the resulting matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = '../01_data/01_biological_data'\n",
    "md_filename = 'metadata_chile.tsv'\n",
    "md_path = os.path.join(md_dir,md_filename)\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n",
    "\n",
    "output_dir = md_dir \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_clean = md[md['Depth level']=='SRF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(md_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in matrix_files:\n",
    "    print(f\"filtering {name}\")\n",
    "    file_path = os.path.join(md_dir, name)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    clean_matrix = md_clean.join(matrix).drop(md_clean.columns,axis = 1)\n",
    "    output_filename =  name[:-8] + '_srf.tsv'\n",
    "    clean_matrix.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "all_metrics_results = []\n",
    "clustering_results_dict = {}\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('_srf.tsv')])\n",
    "\n",
    "# Perform K-Means for different n-clusters for each matrix\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "for matrix_file in matrix_files:\n",
    "    print(f\"performing k-means to {matrix_file}\")\n",
    "    file_path = os.path.join(input_dir, matrix_file)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    matrix_type_subsample = \"_\".join(base_filename.split('_')[3:])\n",
    "    \n",
    "    perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "    # CLR normalized matrix clustering\n",
    "    clr_matrix = clr_(matrix)\n",
    "    perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "\n",
    "\n",
    "combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "#combined_clustering_results = combined_clustering_results.sort_index(axis=1)\n",
    "\n",
    "# Results of the kmeans\n",
    "output_filename = 'kmeans_results_ch_fc.tsv'\n",
    "combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "# Results of the metrics of the kmeans clustering\n",
    "metrics_df = pd.DataFrame(all_metrics_results)\n",
    "metrics_output_filename = 'kmeans_metrics_ch_fc.tsv'\n",
    "metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "# Plot metrics\n",
    "unique_matrices = metrics_df['matrix'].unique()\n",
    "for matrix_type_subsample in unique_matrices:\n",
    "    matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "    ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "    ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "    ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "    ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "    ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f'kmeans_metrics_{matrix_type_subsample}_ch_fc.pdf'\n",
    "    plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_fc.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "new_keys = {col: col+'_fc' for col in clusters.columns}\n",
    "clusters.rename(columns = new_keys, inplace=True)\n",
    "\n",
    "\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Tara Chile\n",
    "In this section we conduct some experiments in order to prepare the XGB study on the Chilean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we range over some selections of hyper-pareters for the XGB method, and use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for file in predictor_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    idx = f\"{file_name}_s{n_splits}_r{n_repeats}\"\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "    aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "\n",
    "    for target_column in columns_to_use:\n",
    "        n_clusters = int(target_column[-1])\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "\n",
    "        y_encoded = le.fit_transform(y)\n",
    "\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[idx, target_column] = (avg_accuracy, avg_f1_macro)\n",
    "                    \n",
    "#print(results_df)\n",
    "\n",
    "\n",
    "#results_df.to_csv('../03_results/out_predictions/predictions_kmeans.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc['matrix_tara_chile_adj_grids_25_all.tsv_s8_r9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to show that the best column to try to predict is `clr_M0_all_kmeans_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'labels'\n",
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    #Cross-val to get optimal n_estimators\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        print(cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(f\"AUC Score (Train): {roc_auc_score(dtrain[target], dtrain_predprob, multi_class = 'ovo')}\")\n",
    "    xgb.plot_importance(alg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    booster = 'gbtree',\n",
    "                                    eval_metric='merror',\n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi:softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=10000,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "modelfit(model_1, train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,20,1),\n",
    " 'min_child_weight':range(1,30,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    "                                    param_grid = param_test1, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch1.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/100.0 for i in range(0,50)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test3, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch3.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(3,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(3,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test4, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch4.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(40,60,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07\n",
    "                                    ), \n",
    " param_grid = param_test5, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch5.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.2,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.55\n",
    "                                    ), \n",
    " param_grid = param_test6, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch6.fit(pred_train,lbs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'learning_rate':[0.1, 0.01,0.001,0.0001]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    ), \n",
    " param_grid = param_test7, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch7.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch7.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eso nos deja con el siguiente estimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.1,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(test[preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtest_predprob = final_model.predict_proba(test[preds])\n",
    "roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n",
      "Mejores hiperparámetros: {'colsample_bytree': 0.4, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 15, 'reg_alpha': 0.05, 'subsample': 0.5}\n",
      "Mejor puntaje: 0.5083466967814795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir el modelo\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [5,8,10,12,15],\n",
    "    'subsample': [0.3, 0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[5*1e-2, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tiempo: 49 min\n",
    "\n",
    "Mejores hiperparámetros: `{'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'reg_alpha': 1e-05, 'subsample': 0.8}`.\n",
    "\n",
    "Mejor puntaje (accuracy): 0.7822656342513322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid2 = {\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'n_estimators': [5, 10 , 15],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search2 = GridSearchCV(estimator=model, param_grid=param_grid2, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search2.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search2.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores hiperparámetros: `{'colsample_bytree': 0.6, 'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 5, 'reg_alpha': 1e-05, 'subsample': 0.8}`\n",
    "\n",
    "Mejor puntaje: 0.7853041164911875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third and last refinement\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.005,  0.006, 0.004],\n",
    "    'n_estimators': [5,6, 7, 8],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.65, 0.55, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'min_child_weight': range(10)\n",
    "}\n",
    "\n",
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.005,  max_depth= 3, n_estimators= 8, reg_alpha= 1e-05, subsample= 0.75)\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=better_model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.1)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7 )\n",
    "better_model.fit(pred_train,lbs_train)\n",
    "xgb.plot_importance(better_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_report(model, test):\n",
    "    y_pred = model.predict(test[preds])\n",
    "    dtest_predprob = better_model.predict_proba(test[preds])\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')}\")\n",
    "    print(f\"f1 score: {f1_score(y_pred,test[target],average='macro')}\")\n",
    "    print(f\"Acc score: {accuracy_score(y_pred,test[target])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "naive_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(naive_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_imp(xgb):\n",
    "    imp_vals = xgb.get_booster().get_fscore()\n",
    "    feats_imp = pd.DataFrame(imp_vals,index=np.arange(2)).T\n",
    "    feats_imp.iloc[:,0]= feats_imp.index    \n",
    "    feats_imp.columns=['feature','importance']\n",
    "    feats_imp.sort_values('importance',inplace=True,ascending=False)\n",
    "    #feats_imp.reset_index(drop=True,inplace=True)\n",
    "    return feats_imp.set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_imps = get_xgb_imp(naive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = list(naive_imps[naive_imps['importance']>= 100].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7)\n",
    "better_model.fit(train.loc[:,important_feats],lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = better_model.predict(test[important_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test[target],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred,test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
