{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary exploration: reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind this notebook is to study the data and compare what we get by calculating things and the corresponding values stored in the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by picking one of the TARA matrices stored in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df1.columns and 'bbp_unc_443' in df1.columns:\n",
    "    df1 = df1.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_25_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df2.columns and 'bbp_unc_443' in df2.columns:\n",
    "    df2= df2.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df1.columns == df2.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (df1.values - df2.values)\n",
    "total_err = diff.sum()\n",
    "print(total_err/df1.values.shape[0]*df1.values.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too high, so lets dig into that error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = diff.nonzero()\n",
    "for k in range(len(idxs[0])):\n",
    "    i = idxs[0][k]\n",
    "    j = idxs[1][k]\n",
    "    print(f\" Difference at ({i},{j}) = {diff[i,j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is only one 'big' error in the last element, meanwhile the rest can be attributed to float point. The numerical coordinates of the attribute are (79,31) as we can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.iloc[79].keys()[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the problematic sample is `TSC280`, and the problematic feature is `PAR.par`. Looking at the individual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Repo-stored value: {df1['PAR.par'].loc['TSC280']}.\\nNew obtained value: {df2['PAR.par'].loc['TSC280']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to recalculate this value 'by hand' and compare it to the two that we have. This will be done with the source code of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "def find_satellite_file(directory, pattern):\n",
    "    regex = re.compile(pattern)\n",
    "    for file in os.listdir(directory):\n",
    "        if regex.match(file):\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "def select_n_nearest_valid(ds, feature, latitude, longitude, n):\n",
    "    latitudes = ds['lat'].values\n",
    "    longitudes = ds['lon'].values\n",
    "    \n",
    "    lat_grid, lon_grid = np.meshgrid(latitudes, longitudes, indexing='ij')\n",
    "    distances = np.sqrt((lat_grid - latitude)**2 + (lon_grid - longitude)**2)\n",
    "    \n",
    "    sorted_indices = np.unravel_index(np.argsort(distances, axis=None), distances.shape)\n",
    "    \n",
    "    valid_points = []\n",
    "    for i in range(len(sorted_indices[0])):\n",
    "        lat_idx = sorted_indices[0][i]\n",
    "        lon_idx = sorted_indices[1][i]\n",
    "        data_point = ds[feature].isel(lat=lat_idx, lon=lon_idx)\n",
    "        if not np.isnan(data_point.values):\n",
    "            valid_points.append(data_point.values)\n",
    "        if len(valid_points) >= n:\n",
    "            break\n",
    "    \n",
    "    if len(valid_points) > 0:\n",
    "        return np.mean(valid_points)\n",
    "    else:\n",
    "        return np.nan\n",
    "n = 25 #number of near points.\n",
    "file_path = '../01_data/01_biological_data'\n",
    "file_name = 'metadata.tsv'\n",
    "sat_data_path = '../01_data/00_satellite_data'\n",
    "\n",
    "output_dir = '../01_data/02_satellite_data_processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file = os.path.join(file_path, file_name)\n",
    "md = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "md_srf = md[md.Layer == 'SRF'].copy()\n",
    "md_srf['Event.date.YYYYMM'] = md_srf['Event.date'].str[:7].str.replace('-', '')\n",
    "md_srf['Event.date.YYYYMM01'] = md_srf['Event.date'].str[:7].str.replace('-', '')+'01'\n",
    "\n",
    "satellite_features = [\n",
    "    'CHL.chlor_a', 'FLH.nflh', 'FLH.ipar', 'IOP.adg_unc_443', 'IOP.adg_443',\n",
    "    'IOP.aph_unc_443', 'IOP.aph_44', 'IOP.bbp_s', 'IOP.adg_s', 'bbp_unc_443', \n",
    "    'IOP.bbp_443', 'IOP.a_412', 'IOP.a_443', 'IOP.a_469', 'IOP.a_488', 'IOP.a_531', \n",
    "    'IOP.a_547', 'IOP.a_555', 'IOP.a_645', 'IOP.a_667', 'IOP.a_678', 'IOP.bb_412', \n",
    "    'IOP.bb_443', 'IOP.bb_469', 'IOP.bb_488', 'IOP.bb_531', 'IOP.bb_547', 'IOP.bb_555', \n",
    "    'IOP.bb_645', 'IOP.bb_667', 'IOP.bb_678', 'KD.Kd_490', 'NSST.sst', 'PAR.par', \n",
    "    'PIC.pic', 'POC.poc', 'RRS.aot_869', 'RRS.angstrom', 'RRS.Rrs_412', 'RRS.Rrs_443', \n",
    "    'RRS.Rrs_469', 'RRS.Rrs_488', 'RRS.Rrs_531', 'RRS.Rrs_547', 'RRS.Rrs_555', \n",
    "    'RRS.Rrs_645', 'RRS.Rrs_667', 'RRS.Rrs_678', 'SST.sst'\n",
    "]\n",
    "\n",
    "satellite_data_terra = pd.DataFrame(index=md_srf.index, columns=satellite_features)\n",
    "satellite_data_aqua = pd.DataFrame(index=md_srf.index, columns=satellite_features)\n",
    "row = md_srf.loc['TSC280'] #We use the obtained key.\n",
    "feature = 'PAR.par' #We use the obtained feature.\n",
    "latitude = row['Latitude']\n",
    "longitude = row['Longitude']\n",
    "date = row['Event.date.YYYYMM']\n",
    "resolution = '9km'\n",
    "pattern_terra = rf\"TERRA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_terra = find_satellite_file(sat_data_path, pattern_terra)\n",
    "pattern_aqua = rf\"AQUA_MODIS\\.{date}01_{date}\\d{{2}}\\.L3m\\.MO\\.{feature}\\.{resolution}\\.nc\"\n",
    "file_path_aqua = find_satellite_file(sat_data_path, pattern_aqua)\n",
    "ds_terra = xr.open_dataset(file_path_terra)\n",
    "ds_aqua = xr.open_dataset(file_path_aqua)\n",
    "terra_values = []\n",
    "aqua_values = []\n",
    "try:\n",
    "    variable_name = feature.split('.')[1]\n",
    "    \n",
    "    data_point_terra = select_n_nearest_valid(ds_terra, variable_name, latitude, longitude, 25)\n",
    "    terra_values.append(data_point_terra)\n",
    "\n",
    "    data_point_aqua = select_n_nearest_valid(ds_aqua, variable_name, latitude, longitude, 25)\n",
    "    aqua_values.append(data_point_aqua)\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Feature {feature} not found in dataset\")\n",
    "finally:\n",
    "    ds_terra.close()\n",
    "    ds_aqua.close()\n",
    "aqua_arr = np.array(aqua_values)\n",
    "terra_arr = np.array(terra_values)\n",
    "final_arr = (aqua_arr + terra_arr)*0.5\n",
    "print(f'Value by hand: {final_arr[0]}.\\nValue stored in repo: {df1['PAR.par'].loc['TSC280']}. \\nNew value obtained by script: {df2['PAR.par'].loc['TSC280']}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means the value obtained by hand is closer to the '_new' value than to the one stored in the repo, with error likely of approximation. The difference of the old value and the new value is of order $10^{-3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us make a simmilar study to the `n=1` case: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01.tsv\", sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df3.columns and 'bbp_unc_443' in df3.columns:\n",
    "    df3 = df3.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define it's 'new' counterpart, obtained in 28/10 via the `00_00_extract_satellite_data.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_world_adj_grids_01_new.tsv', sep='\\t', index_col=0)\n",
    "if 'IOP.aph_44' in df4.columns and 'bbp_unc_443' in df4.columns:\n",
    "    df4= df4.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking the shape of the DataFrames match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df3.columns == df4.columns).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the values are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = (df3.values - df4.values)\n",
    "print(diff2.max())\n",
    "print(diff2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_err2 = diff2.sum()\n",
    "print(total_err2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the values are all equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the impression that the issue is somewhere between the part where the closest sat reads (physicall distance-wise) are chosen. There is a pending plan of testing with a third run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate further one may plot the points selected in the proccess for both versions and check if they are the same or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might also be related to eventual changes to the original database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
