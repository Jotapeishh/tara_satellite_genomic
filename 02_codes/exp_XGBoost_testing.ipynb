{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Tara Chile\n",
    "In this section we conduct some experiments in order to prepare the XGB study on the Chilean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we range over some selections of hyper-pareters for the XGB method, and use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for file in predictor_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    idx = f\"{file_name}_s{n_splits}_r{n_repeats}\"\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "    aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "\n",
    "    for target_column in columns_to_use:\n",
    "        n_clusters = int(target_column[-1])\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "\n",
    "        y_encoded = le.fit_transform(y)\n",
    "\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[idx, target_column] = (avg_accuracy, avg_f1_macro)\n",
    "                    \n",
    "#print(results_df)\n",
    "\n",
    "\n",
    "#results_df.to_csv('../03_results/out_predictions/predictions_kmeans.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to show that the best column to try to predict is `clr_M0_all_kmeans_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'labels'\n",
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    #Cross-val to get optimal n_estimators\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        print(cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(f\"AUC Score (Train): {roc_auc_score(dtrain[target], dtrain_predprob, multi_class = 'ovo')}\")\n",
    "    xgb.plot_importance(alg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    booster = 'gbtree',\n",
    "                                    eval_metric='merror',\n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi:softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=10000,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "modelfit(model_1, train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,20,1),\n",
    " 'min_child_weight':range(1,30,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    "                                    param_grid = param_test1, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch1.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/100.0 for i in range(0,50)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test3, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch3.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(3,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(3,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test4, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch4.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(40,60,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07\n",
    "                                    ), \n",
    " param_grid = param_test5, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch5.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.2,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.55\n",
    "                                    ), \n",
    " param_grid = param_test6, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch6.fit(pred_train,lbs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'learning_rate':[0.1, 0.01,0.001,0.0001]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    ), \n",
    " param_grid = param_test7, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch7.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch7.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eso nos deja con el siguiente estimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.1,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(test[preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtest_predprob = final_model.predict_proba(test[preds])\n",
    "roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el modelo\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [5,8,10,12,15],\n",
    "    'subsample': [0.3, 0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[5*1e-2, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tiempo: 49 min\n",
    "\n",
    "Mejores hiperparámetros: `{'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'reg_alpha': 1e-05, 'subsample': 0.8}`.\n",
    "\n",
    "Mejor puntaje (accuracy): 0.7822656342513322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid2 = {\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'n_estimators': [5, 10 , 15],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search2 = GridSearchCV(estimator=model, param_grid=param_grid2, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search2.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search2.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores hiperparámetros: `{'colsample_bytree': 0.6, 'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 5, 'reg_alpha': 1e-05, 'subsample': 0.8}`\n",
    "\n",
    "Mejor puntaje: 0.7853041164911875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third and last refinement\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.005,  0.006, 0.004],\n",
    "    'n_estimators': [5,6, 7, 8],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.65, 0.55, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'min_child_weight': range(10)\n",
    "}\n",
    "\n",
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.005,  max_depth= 3, n_estimators= 8, reg_alpha= 1e-05, subsample= 0.75)\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=better_model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.1)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7 )\n",
    "better_model.fit(pred_train,lbs_train)\n",
    "xgb.plot_importance(better_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_report(model, test):\n",
    "    y_pred = model.predict(test[preds])\n",
    "    dtest_predprob = better_model.predict_proba(test[preds])\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')}\")\n",
    "    print(f\"f1 score: {f1_score(y_pred,test[target],average='macro')}\")\n",
    "    print(f\"Acc score: {accuracy_score(y_pred,test[target])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "naive_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(naive_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_imp(xgb):\n",
    "    imp_vals = xgb.get_booster().get_fscore()\n",
    "    feats_imp = pd.DataFrame(imp_vals,index=np.arange(2)).T\n",
    "    feats_imp.iloc[:,0]= feats_imp.index    \n",
    "    feats_imp.columns=['feature','importance']\n",
    "    feats_imp.sort_values('importance',inplace=True,ascending=False)\n",
    "    #feats_imp.reset_index(drop=True,inplace=True)\n",
    "    return feats_imp.set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_imps = get_xgb_imp(naive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = list(naive_imps[naive_imps['importance']>= 100].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7)\n",
    "better_model.fit(train.loc[:,important_feats],lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = better_model.predict(test[important_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test[target],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred,test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "cl_path = '../03_results/out_genomic_clusters/kmeans_results_ch.tsv'\n",
    "cl_df = pd.read_csv(cl_path, sep = \"\\t\")\n",
    "\n",
    "#Export to get datased to plot in 3D\n",
    "cols_to_get = cl_df.columns.to_list() + ['lat_cast','lon_cast', 'Depth [m]']\n",
    "file = pd.merge(md_df, cl_df, on='Samples')[cols_to_get]\n",
    "file.to_csv(path_or_buf='../03_results/clusters_with_coords.tsv', sep= '\\t')\n",
    "\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "cl_df.set_index('Samples', inplace=True)\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df = md_df[['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]', 'Depth level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df['Depth level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for the main metadata feats ordered from smaller to larger value.\n",
    "xvals = {'all': 148, 'SRF': 27, 'MES': 38, 'EPI': 81}\n",
    "spaces = {'Temperature [ºC]':5,'Oxygen [ml/l]':7,'nitrates [uM]':5}\n",
    "for col in md_df.columns:\n",
    "    if col != 'Depth level':\n",
    "        space = spaces[col]\n",
    "        xval = xvals['all']\n",
    "        data = md_df[col].sort_values()\n",
    "        fig, ax = plt.subplots(figsize = (25,15))\n",
    "        ax.set_title(f'{col} values in TARA Chile (all)', fontsize = 35, pad = 20)\n",
    "        ax.set_xticks([])\n",
    "        plt.yticks(fontsize = 15)\n",
    "        plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "        plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "        ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "        ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "        ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "        plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "        plt.scatter(data.index,data)\n",
    "        sns.set(style='darkgrid')\n",
    "        path = f'../03_results/out_ch_data_analysis/{col[:-space]}_all'\n",
    "        plt.savefig(path)\n",
    "        for depth in md_df['Depth level'].unique():\n",
    "            #plot by depth level\n",
    "            xval = xvals[depth]\n",
    "            data = (md_df[md_df['Depth level'] == depth])[col].sort_values()\n",
    "            fig, ax = plt.subplots(figsize = (25,15))\n",
    "            ax.set_title(f'{col} values in TARA Chile ({depth})', fontsize = 35, pad = 20)\n",
    "            ax.set_xticks([])\n",
    "            plt.yticks(fontsize = 15)\n",
    "            plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "            plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "            ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "            ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "            ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "            plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "            plt.scatter(data.index,data)\n",
    "            sns.set(style='darkgrid')\n",
    "            path = f'../03_results/out_ch_data_analysis/{col[:-4]}_{depth}'\n",
    "            plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata quantile-based binning\n",
    "n_bins = [3,4,5,6,7,8] \n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all','SRF','EPI','MES']\n",
    "for n in n_bins:\n",
    "    q = 1/n\n",
    "    for layer in layers:\n",
    "        if layer == 'all':\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[feat] \n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df[binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")\n",
    "        else:\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[md_df['Depth level'] == layer][feat]\n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df.loc[md_df['Depth level'] == layer,binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df\n",
    "for layer in layers:\n",
    "    if layer == 'all':\n",
    "        data = md_df[[col for col in md_df.columns if layer in col]]\n",
    "    else:\n",
    "        data = md_df[md_df['Depth level'] == layer][[col for col in md_df.columns if layer in col]]\n",
    "    data.to_csv(path_or_buf= f'../03_results/metadata_based_clusters/metadata_clusters_{layer}.tsv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = '../03_results/metadata_based_clusters'\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all'\n",
    "#          ,'SRF','EPI','MES'\n",
    "          ]\n",
    "columns_to_use = []\n",
    "for feat in feats:\n",
    "    clean_feat = feat.split(\" \", 1)[0]\n",
    "    for n in desired_clusters:\n",
    "        columns_to_use.append(clean_feat+'_'+n)\n",
    "\n",
    "results_df = pd.DataFrame(index=layers, columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for target_vars_filename in [f for f in os.listdir(cluster_dir) if not f.split('_')[-1] == 'metrics.tsv']:\n",
    "    target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "    target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    layer = target_vars_filename[-7:-4]\n",
    "    for col in columns_to_use:\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        feat = col.split('_')[0]\n",
    "        target_column = f\"{feat}_{n_clusters}_{layer}\"\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[layer, col] = f\"({avg_accuracy}, {avg_f1_macro})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[col for col in results_df.columns if '8' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv(path_or_buf='../03_results/metadata_based_clusters/metadata_cluster_metrics_splitted.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup: Metadata +  Biodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try another approach to the NASA-based bio-prediction concept. Now we will mix the metadata with the biodata, and based on that we will cluster with k-means (in contrast to the only bio-based k-means done previously), and then we will try to predict those clusters using the NASA data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "cl_path = '../03_results/out_genomic_clusters/kmeans_results_ch.tsv'\n",
    "cl_df = pd.read_csv(cl_path, sep = \"\\t\")\n",
    "\n",
    "#Export to get datased to plot in 3D\n",
    "cols_to_get = cl_df.columns.to_list() + ['lat_cast','lon_cast', 'Depth [m]']\n",
    "file = pd.merge(md_df, cl_df, on='Samples')[cols_to_get]\n",
    "file.to_csv(path_or_buf='../03_results/clusters_with_coords.tsv', sep= '\\t')\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "cl_df.set_index('Samples', inplace=True)\n",
    "usable_cols = [i for i in md_df.columns if md_df.dtypes.loc[i] in [float,int]]\n",
    "cols = [c for c in usable_cols if md_df.min()[c]>0]\n",
    "md_df = md_df[cols]\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df.drop(columns=['Nitrate [uM]','Nitrates [uM]'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Leg', 'Station', 'Depth [m]', 'Temperature [ºC]', 'Salinity [PSU]',\n",
       "       'Density [kg/m3]', 'Fluorescence [mg/m3]', 'Orthophosphate [uM]',\n",
       "       'Silicic-acid [uM]', 'Nitrite [uM]', 'NP ratio', 'year', 'month', 'day',\n",
       "       'hour', 'minute', 'second', 'nitrates [uM]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataframes based on which the clusters will be made, taking care in eliminating unnecessary columns. For that, we firstly drop all the non-important technical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 865)\n",
      "(159, 37853)\n",
      "(159, 10244)\n",
      "(159, 66)\n",
      "(159, 72)\n"
     ]
    }
   ],
   "source": [
    "bio_path = '../01_data/01_biological_data'\n",
    "path_list = [path for path in os.listdir(bio_path) if 'Matrix_chile' in path and '_all.tsv' in path]\n",
    "df_list = []\n",
    "for path in path_list:\n",
    "    full_path = f\"{bio_path}/{path}\"\n",
    "    bio_df = pd.read_csv(full_path, sep = '\\t').set_index('Samples')\n",
    "    full_df = md_df.join(bio_df)\n",
    "    final_df = full_df.drop(columns=[c for c in md_df.columns if c in ['Leg', 'Station', 'Station ID', 'Depth ID', 'lat_cast',\n",
    "       'lon_cast', 'datetime', 'Depth [m]', 'instrument','original file', 'year', 'month', 'day', 'hour', 'minute',\n",
    "       'second']])\n",
    "    df_list.append(final_df)\n",
    "    print(final_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we parse the dataframes looking for the columns with no variability, and drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 48/865 [00:00<00:00, 101577.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_guidi_all.tsv: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3680/37853 [00:00<00:00, 233563.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_M0_all.tsv: 3680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 487/10244 [00:00<00:00, 206305.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_M1_all.tsv: 487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/66 [00:00<00:00, 12610.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_salazar_all.tsv: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/72 [00:00<00:00, 2746.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_stress_all.tsv: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trivial_keys = {}\n",
    "for k in range(len(df_list)):\n",
    "    df = df_list[k]\n",
    "    ordered = df.nunique().sort_values().copy(deep = True)\n",
    "    for key in tqdm(ordered.index):\n",
    "        if ordered[key] > 1:\n",
    "            break\n",
    "    first_non_triv_key = key\n",
    "    first_non_triv_ind = ordered.index.get_loc(first_non_triv_key)\n",
    "    triv_keys = ordered.index[:first_non_triv_ind]\n",
    "    trivial_keys[path_list[k]] = triv_keys\n",
    "    print(f\"Number of columns dropped from {path_list[k]}: {len(trivial_keys[path_list[k]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Matrix_chile_GEN_guidi_all.tsv shape: (159, 865)\n",
      "New Matrix_chile_GEN_guidi_all.tsv shape: (159, 817)\n",
      "Old Matrix_chile_GEN_M0_all.tsv shape: (159, 37853)\n",
      "New Matrix_chile_GEN_M0_all.tsv shape: (159, 34173)\n",
      "Old Matrix_chile_GEN_M1_all.tsv shape: (159, 10244)\n",
      "New Matrix_chile_GEN_M1_all.tsv shape: (159, 9757)\n",
      "Old Matrix_chile_GEN_salazar_all.tsv shape: (159, 66)\n",
      "New Matrix_chile_GEN_salazar_all.tsv shape: (159, 61)\n",
      "Old Matrix_chile_GEN_stress_all.tsv shape: (159, 72)\n",
      "New Matrix_chile_GEN_stress_all.tsv shape: (159, 71)\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(df_list)):\n",
    "    matrix_path = path_list[k]\n",
    "    df = df_list[k]\n",
    "    print(f\"Old {matrix_path} shape: {df.shape}\")\n",
    "    df_list[k].drop(columns = trivial_keys[path_list[k]], inplace = True)\n",
    "    print(f\"New {matrix_path} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "\n",
    "all_metrics_results = []\n",
    "clustering_results_dict = {}\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix created\n",
      "Plotting metrics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Perform K-Means for different n-clusters for each matrix\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "for name, matrix in zip(path_list,df_list):\n",
    "    matrix_type_subsample = name[17:-4]\n",
    "    perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "    # CLR normalized matrix clustering\n",
    "    clr_matrix = clr_(matrix)\n",
    "    perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "print('Matrix created')\n",
    "\n",
    "combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "\n",
    "# Results of the kmeans\n",
    "output_filename = 'kmeans_results_metabio_ch.tsv'\n",
    "combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "# Results of the metrics of the kmeans clustering\n",
    "metrics_df = pd.DataFrame(all_metrics_results)\n",
    "metrics_output_filename = 'kmeans_metrics_metabio_ch.tsv'\n",
    "metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "# Plot metrics\n",
    "unique_matrices = metrics_df['matrix'].unique()\n",
    "print('Plotting metrics.')\n",
    "for matrix_type_subsample in tqdm(unique_matrices):\n",
    "    matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "    ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "    ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "    ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "    ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "    ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f'kmeans_metrics_{matrix_type_subsample}_metabio_ch.pdf'\n",
    "    plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:46<00:00,  8.35s/it]\n"
     ]
    }
   ],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = '../03_results/out_genomic_clusters/clusters_ch/metabio_clusters'\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all'\n",
    "#          ,'SRF','EPI','MES'\n",
    "          ]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "target_vars_filename = 'kmeans_results_metabio_ch.tsv'\n",
    "target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "\n",
    "#DataFrame with the clusters\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "results_df = pd.DataFrame(columns=columns_to_use)\n",
    "\n",
    "aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "layer = target_vars_filename[-7:-4]\n",
    "for col in tqdm(columns_to_use):\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'f1_macro': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "\n",
    "    n_clusters = int(col.split('_')[-1])\n",
    "    X = aligned_predictor\n",
    "    y = target_vars.loc[aligned_predictor.index, col]\n",
    "    non_nan_indices = y.dropna().index\n",
    "    X = X.loc[non_nan_indices]\n",
    "    y = y.loc[non_nan_indices]\n",
    "    \n",
    "    y_encoded = le.fit_transform(y)\n",
    "    unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "    min_samples = n_splits\n",
    "\n",
    "    X_resampled = X.copy()\n",
    "    y_resampled = y_encoded.copy()\n",
    "\n",
    "    for cls, count in zip(unique, counts):\n",
    "        if count < min_samples:\n",
    "            diff = min_samples - count\n",
    "            cls_indices = np.where(y_encoded == cls)[0]\n",
    "            indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "            X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "            y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "    model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                seed = 29,\n",
    "                                objective= 'multi: softmax',\n",
    "                                num_class = n_clusters,\n",
    "                                learning_rate =0.2,\n",
    "                                n_estimators=10,\n",
    "                                max_depth=5,\n",
    "                                min_child_weight=1,\n",
    "                                gamma=0,\n",
    "                                subsample=0.8,\n",
    "                                colsample_bytree=0.8\n",
    "                                )\n",
    "\n",
    "    #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "    cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "    avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "    avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "    results_df.at[layer, col] = f\"({avg_accuracy}, {avg_f1_macro})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(path_or_buf='../03_results/out_predictions/predictions_kmeans_metabio.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
