{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Tara Chile\n",
    "In this section we conduct some experiments in order to prepare the XGB study on the Chilean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we range over some selections of hyper-pareters for the XGB method, and use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for file in predictor_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    idx = f\"{file_name}_s{n_splits}_r{n_repeats}\"\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "    aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "\n",
    "    for target_column in columns_to_use:\n",
    "        n_clusters = int(target_column[-1])\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "\n",
    "        y_encoded = le.fit_transform(y)\n",
    "\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[idx, target_column] = (avg_accuracy, avg_f1_macro)\n",
    "                    \n",
    "#print(results_df)\n",
    "\n",
    "\n",
    "#results_df.to_csv('../03_results/out_predictions/predictions_kmeans.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to show that the best column to try to predict is `clr_M0_all_kmeans_5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'labels'\n",
    "def modelfit(alg, dtrain, predictors, useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    #Cross-val to get optimal n_estimators\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        print(cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[target])\n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "    print(f\"AUC Score (Train): {roc_auc_score(dtrain[target], dtrain_predprob, multi_class = 'ovo')}\")\n",
    "    xgb.plot_importance(alg)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    booster = 'gbtree',\n",
    "                                    eval_metric='merror',\n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi:softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=10000,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "modelfit(model_1, train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,20,1),\n",
    " 'min_child_weight':range(1,30,1)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    "                                    param_grid = param_test1, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch1.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/100.0 for i in range(0,50)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test3, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch3.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(3,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(3,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    ), \n",
    " param_grid = param_test4, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch4.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(80,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(40,60,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.07\n",
    "                                    ), \n",
    " param_grid = param_test5, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch5.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=7,\n",
    "                                    gamma=0.2,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.55\n",
    "                                    ), \n",
    " param_grid = param_test6, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch6.fit(pred_train,lbs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'learning_rate':[0.1, 0.01,0.001,0.0001]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.01,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    ), \n",
    " param_grid = param_test7, scoring='roc_auc_ovo',n_jobs=1, cv=5)\n",
    "gsearch7.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gsearch7.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eso nos deja con el siguiente estimador:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                    eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = 5,\n",
    "                                    learning_rate =0.1,\n",
    "                                    n_estimators=1,\n",
    "                                    max_depth=3,\n",
    "                                    min_child_weight=4,\n",
    "                                    gamma=0.37,\n",
    "                                    subsample=0.9,\n",
    "                                    colsample_bytree=0.2,\n",
    "                                    reg_alpha = 1e-05\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(test[preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtest_predprob = final_model.predict_proba(test[preds])\n",
    "roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaned up version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.3)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir el modelo\n",
    "model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [5,8,10,12,15],\n",
    "    'subsample': [0.3, 0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[5*1e-2, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1_macro', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tiempo: 49 min\n",
    "\n",
    "Mejores hiperparámetros: `{'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'reg_alpha': 1e-05, 'subsample': 0.8}`.\n",
    "\n",
    "Mejor puntaje (accuracy): 0.7822656342513322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refinement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid2 = {\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.005, 0.01, 0.05],\n",
    "    'n_estimators': [5, 10 , 15],\n",
    "    'subsample': [0.6, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search2 = GridSearchCV(estimator=model, param_grid=param_grid2, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search2.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search2.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejores hiperparámetros: `{'colsample_bytree': 0.6, 'learning_rate': 0.005, 'max_depth': 3, 'n_estimators': 5, 'reg_alpha': 1e-05, 'subsample': 0.8}`\n",
    "\n",
    "Mejor puntaje: 0.7853041164911875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third and last refinement\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.005,  0.006, 0.004],\n",
    "    'n_estimators': [5,6, 7, 8],\n",
    "    'subsample': [0.75, 0.8, 0.85],\n",
    "    'colsample_bytree': [0.65, 0.55, 0.6],\n",
    "    'reg_alpha':[1e-5]\n",
    "}\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_child_weight\n",
    "# Definir los hiperparámetros a evaluar\n",
    "param_grid3 = {\n",
    "    'min_child_weight': range(10)\n",
    "}\n",
    "\n",
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.005,  max_depth= 3, n_estimators= 8, reg_alpha= 1e-05, subsample= 0.75)\n",
    "\n",
    "# Configurar la búsqueda en cuadrícula\n",
    "grid_search3 = GridSearchCV(estimator=better_model, param_grid=param_grid3, cv=5, scoring='roc_auc_ovr', verbose=1)\n",
    "\n",
    "# Entrenar y buscar los mejores parámetros\n",
    "grid_search3.fit(pred_train, lbs_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", grid_search3.best_params_)\n",
    "print(\"Mejor puntaje:\", grid_search3.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "\n",
    "input_kmeans_dir = '../03_results/out_genomic_clusters'\n",
    "target_vars_filename = 'kmeans_results_ch.tsv'\n",
    "target_vars_path = os.path.join(input_kmeans_dir, target_vars_filename)\n",
    "\n",
    "target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "#target_vars.head()\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters\n",
    "columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(index=[os.path.basename(file) for file in predictor_files], columns=columns_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = target_vars['clr_M0_all_kmeans_5']\n",
    "file = f\"../01_data/02_satellite_data_processed/{desired_files[0]}\"\n",
    "df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "aligned_predictor = df.loc[df.index.intersection(target_vars.index)] # satellite\n",
    "lbs = labels.loc[aligned_predictor.index]\n",
    "lbs = lbs.map(lambda x: int(f\"{x[1:]}\"))\n",
    "aligned_predictor = aligned_predictor.drop(columns = ['IOP.aph_44','bbp_unc_443'])\n",
    "target = 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = aligned_predictor.copy()\n",
    "full_data[target] = lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = aligned_predictor.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(full_data, test_size= 0.1)\n",
    "pred_train, lbs_train =  train[preds], train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7 )\n",
    "better_model.fit(pred_train,lbs_train)\n",
    "xgb.plot_importance(better_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_report(model, test):\n",
    "    y_pred = model.predict(test[preds])\n",
    "    dtest_predprob = better_model.predict_proba(test[preds])\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(test[target], dtest_predprob, multi_class = 'ovr')}\")\n",
    "    print(f\"f1 score: {f1_score(y_pred,test[target],average='macro')}\")\n",
    "    print(f\"Acc score: {accuracy_score(y_pred,test[target])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "naive_model.fit(pred_train,lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(naive_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_imp(xgb):\n",
    "    imp_vals = xgb.get_booster().get_fscore()\n",
    "    feats_imp = pd.DataFrame(imp_vals,index=np.arange(2)).T\n",
    "    feats_imp.iloc[:,0]= feats_imp.index    \n",
    "    feats_imp.columns=['feature','importance']\n",
    "    feats_imp.sort_values('importance',inplace=True,ascending=False)\n",
    "    #feats_imp.reset_index(drop=True,inplace=True)\n",
    "    return feats_imp.set_index('feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_imps = get_xgb_imp(naive_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feats = list(naive_imps[naive_imps['importance']>= 100].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better_model = xgb.XGBClassifier(eval_metric='mlogloss', colsample_bytree = 0.55,  learning_rate= 0.001,  max_depth= 3, n_estimators= 5, reg_alpha= 1e-02, subsample= 0.5, min_child_weight = 7)\n",
    "better_model.fit(train.loc[:,important_feats],lbs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_report(better_model,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = better_model.predict(test[important_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test[target],y_pred,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred,test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD-based clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "cl_path = '../03_results/out_genomic_clusters/kmeans_results_ch.tsv'\n",
    "cl_df = pd.read_csv(cl_path, sep = \"\\t\")\n",
    "\n",
    "#Export to get datased to plot in 3D\n",
    "cols_to_get = cl_df.columns.to_list() + ['lat_cast','lon_cast', 'Depth [m]']\n",
    "file = pd.merge(md_df, cl_df, on='Samples')[cols_to_get]\n",
    "file.to_csv(path_or_buf='../03_results/clusters_with_coords.tsv', sep= '\\t')\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "cl_df.set_index('Samples', inplace=True)\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df = md_df[['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]', 'Depth level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df['Depth level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for the main metadata feats ordered from smaller to larger value.\n",
    "xvals = {'all': 148, 'SRF': 27, 'MES': 38, 'EPI': 81}\n",
    "spaces = {'Temperature [ºC]':5,'Oxygen [ml/l]':7,'nitrates [uM]':5}\n",
    "for col in md_df.columns:\n",
    "    if col != 'Depth level':\n",
    "        space = spaces[col]\n",
    "        xval = xvals['all']\n",
    "        data = md_df[col].sort_values()\n",
    "        fig, ax = plt.subplots(figsize = (25,15))\n",
    "        ax.set_title(f'{col} values in TARA Chile (all)', fontsize = 35, pad = 20)\n",
    "        ax.set_xticks([])\n",
    "        plt.yticks(fontsize = 15)\n",
    "        plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "        plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "        ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "        ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "        ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "        plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "        plt.scatter(data.index,data)\n",
    "        sns.set(style='darkgrid')\n",
    "        path = f'../03_results/out_ch_data_analysis/{col[:-space]}_all'\n",
    "        plt.savefig(path)\n",
    "        for depth in md_df['Depth level'].unique():\n",
    "            #plot by depth level\n",
    "            xval = xvals[depth]\n",
    "            data = (md_df[md_df['Depth level'] == depth])[col].sort_values()\n",
    "            fig, ax = plt.subplots(figsize = (25,15))\n",
    "            ax.set_title(f'{col} values in TARA Chile ({depth})', fontsize = 35, pad = 20)\n",
    "            ax.set_xticks([])\n",
    "            plt.yticks(fontsize = 15)\n",
    "            plt.axhline(data.mean(), linestyle='--', color = 'r', label = 'Mean')\n",
    "            plt.axhline(data.median(), linestyle='--', color = 'g', label = 'Median')\n",
    "            ax.set_ylabel(f'{col}', fontsize = 25,labelpad=20)\n",
    "            ax.set_xlabel('Sample', fontsize = 25,labelpad=20)\n",
    "            ax.legend(title=\"MTCs\", fontsize = 15, title_fontsize = 18)\n",
    "            plt.text(xval, data.min(), f'Min: {data.min():.3f}\\nMax: {data.max():.3f}\\nMean: {data.mean():.3f}\\nMedian: {data.median():.3f}', fontsize = 20, bbox = dict(facecolor = 'green', alpha = 0.2, ec = 'black'))\n",
    "            plt.scatter(data.index,data)\n",
    "            sns.set(style='darkgrid')\n",
    "            path = f'../03_results/out_ch_data_analysis/{col[:-4]}_{depth}'\n",
    "            plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata quantile-based binning\n",
    "n_bins = [3,4,5,6,7,8] \n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all','SRF','EPI','MES']\n",
    "for n in n_bins:\n",
    "    q = 1/n\n",
    "    for layer in layers:\n",
    "        if layer == 'all':\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[feat] \n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df[binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")\n",
    "        else:\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[md_df['Depth level'] == layer][feat]\n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df.loc[md_df['Depth level'] == layer,binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "                print(f\"{binning}: {md_df[binning].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df\n",
    "for layer in layers:\n",
    "    if layer == 'all':\n",
    "        data = md_df[[col for col in md_df.columns if layer in col]]\n",
    "    else:\n",
    "        data = md_df[md_df['Depth level'] == layer][[col for col in md_df.columns if layer in col]]\n",
    "    data.to_csv(path_or_buf= f'../03_results/metadata_based_clusters/metadata_clusters_{layer}.tsv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = '../03_results/metadata_based_clusters'\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all'\n",
    "#          ,'SRF','EPI','MES'\n",
    "          ]\n",
    "columns_to_use = []\n",
    "for feat in feats:\n",
    "    clean_feat = feat.split(\" \", 1)[0]\n",
    "    for n in desired_clusters:\n",
    "        columns_to_use.append(clean_feat+'_'+n)\n",
    "\n",
    "results_df = pd.DataFrame(index=layers, columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for target_vars_filename in [f for f in os.listdir(cluster_dir) if not f.split('_')[-1] == 'metrics.tsv']:\n",
    "    target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "    target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    layer = target_vars_filename[-7:-4]\n",
    "    for col in columns_to_use:\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        feat = col.split('_')[0]\n",
    "        target_column = f\"{feat}_{n_clusters}_{layer}\"\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df.at[layer, col] = f\"({avg_accuracy}, {avg_f1_macro})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[[col for col in results_df.columns if '8' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_df.to_csv(path_or_buf='../03_results/metadata_based_clusters/metadata_cluster_metrics_splitted.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixup: Metadata +  Biodata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try another approach to the NASA-based bio-prediction concept. Now we will mix the metadata with the biodata, and based on that we will cluster with k-means (in contrast to the only bio-based k-means done previously), and then we will try to predict those clusters using the NASA data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "cl_path = '../03_results/out_genomic_clusters/kmeans_results_ch.tsv'\n",
    "cl_df = pd.read_csv(cl_path, sep = \"\\t\")\n",
    "\n",
    "#Export to get datased to plot in 3D\n",
    "cols_to_get = cl_df.columns.to_list() + ['lat_cast','lon_cast', 'Depth [m]']\n",
    "file = pd.merge(md_df, cl_df, on='Samples')[cols_to_get]\n",
    "file.to_csv(path_or_buf='../03_results/clusters_with_coords.tsv', sep= '\\t')\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "cl_df.set_index('Samples', inplace=True)\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df.drop(columns=['Nitrate [uM]','Nitrates [uM]'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SAMEA ID', 'Leg', 'Station', 'Station ID', 'Depth ID', 'lat_cast',\n",
       "       'lon_cast', 'datetime', 'Depth [m]', 'Temperature [ºC]',\n",
       "       'Salinity [PSU]', 'Density [kg/m3]', 'Oxygen [ml/l]', 'Oxygen [%]',\n",
       "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
       "       'Nitrite [uM]', 'NP ratio', 'year', 'month', 'day', 'hour', 'minute',\n",
       "       'second', 'instrument', 'original file', 'Depth level', 'Oxygen level',\n",
       "       'Biogeographical units', 'Freshwater inputs', 'Oxy_depth',\n",
       "       'Distance from coast (km)', 'Latitude Bin', 'nitrates [uM]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataframes based on which the clusters will be made,taking care in eliminating unnecessary columns. For that, we firstly drop all the non-important technical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159, 874)\n",
      "(159, 37862)\n",
      "(159, 10253)\n",
      "(159, 75)\n",
      "(159, 81)\n"
     ]
    }
   ],
   "source": [
    "bio_path = '../01_data/01_biological_data'\n",
    "path_list = [path for path in os.listdir(bio_path) if 'Matrix_chile' in path and '_all.tsv' in path]\n",
    "df_list = []\n",
    "for path in path_list:\n",
    "    full_path = f\"{bio_path}/{path}\"\n",
    "    bio_df = pd.read_csv(full_path, sep = '\\t').set_index('Samples')\n",
    "    full_df = md_df.join(bio_df)\n",
    "    final_df = full_df.drop(columns=['SAMEA ID','Leg', 'Station', 'Station ID', 'Depth ID', 'lat_cast',\n",
    "       'lon_cast', 'datetime', 'Depth [m]', 'instrument','original file', 'year', 'month', 'day', 'hour', 'minute',\n",
    "       'second'])\n",
    "    df_list.append(final_df)\n",
    "    print(final_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we parse the dataframes looking for the columns with no variability, and drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 48/874 [00:00<00:00, 43129.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_guidi_all.tsv: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 3680/37862 [00:00<00:00, 111802.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_M0_all.tsv: 3680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 487/10253 [00:00<00:00, 174927.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_M1_all.tsv: 487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 5/75 [00:00<00:00, 1572.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_salazar_all.tsv: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/81 [00:00<00:00, 2896.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns dropped from Matrix_chile_GEN_stress_all.tsv: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trivial_keys = {}\n",
    "for k in range(len(df_list)):\n",
    "    df = df_list[k]\n",
    "    ordered = df.nunique().sort_values().copy(deep = True)\n",
    "    for key in tqdm(ordered.index):\n",
    "        if ordered[key] > 1:\n",
    "            break\n",
    "    first_non_triv_key = key\n",
    "    first_non_triv_ind = ordered.index.get_loc(first_non_triv_key)\n",
    "    triv_keys = ordered.index[:first_non_triv_ind]\n",
    "    trivial_keys[path_list[k]] = triv_keys\n",
    "    print(f\"Number of columns dropped from {path_list[k]}: {len(trivial_keys[path_list[k]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Matrix_chile_GEN_guidi_all.tsv shape: (159, 874)\n",
      "New Matrix_chile_GEN_guidi_all.tsv shape: (159, 826)\n",
      "Old Matrix_chile_GEN_M0_all.tsv shape: (159, 37862)\n",
      "New Matrix_chile_GEN_M0_all.tsv shape: (159, 34182)\n",
      "Old Matrix_chile_GEN_M1_all.tsv shape: (159, 10253)\n",
      "New Matrix_chile_GEN_M1_all.tsv shape: (159, 9766)\n",
      "Old Matrix_chile_GEN_salazar_all.tsv shape: (159, 75)\n",
      "New Matrix_chile_GEN_salazar_all.tsv shape: (159, 70)\n",
      "Old Matrix_chile_GEN_stress_all.tsv shape: (159, 81)\n",
      "New Matrix_chile_GEN_stress_all.tsv shape: (159, 80)\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(df_list)):\n",
    "    matrix_path = path_list[k]\n",
    "    df = df_list[k]\n",
    "    print(f\"Old {matrix_path} shape: {df.shape}\")\n",
    "    df_list[k].drop(columns = trivial_keys[path_list[k]], inplace = True)\n",
    "    print(f\"New {matrix_path} shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "df = df_list[k]\n",
    "print(f\"File: {path_list[k]}\")\n",
    "trivial_keys[path_list[k]] = []\n",
    "for key in tqdm(df.nunique().index):\n",
    "    if df.nunique()[key] == 1:\n",
    "        trivial_keys[path_list[k]].append(key) \n",
    "print(f\"Number of columns dropped from {path_list[k]}: {len(trivial_keys[path_list[k]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "df = df_list[k]\n",
    "ordered = df.nunique().sort_values().copy(deep = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
