{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The idea of this notebook is to get a uniformized and sef-contained pipeline in which we will get the best (most predictable) clustering for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bio data k-means\n",
    "## Data generation and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = '../01_data/01_biological_data/tara_ch'\n",
    "md_filename = 'metadata_chile.tsv'\n",
    "md_path = os.path.join(md_dir,md_filename)\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n",
    "output_dir = md_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering Matrix_chile_GEN_M0_all.tsv\n",
      "filtering Matrix_chile_GEN_M1_all.tsv\n",
      "filtering Matrix_chile_GEN_guidi_all.tsv\n",
      "filtering Matrix_chile_GEN_salazar_all.tsv\n",
      "filtering Matrix_chile_GEN_stress_all.tsv\n",
      "filtering Matrix_chile_GEN_M0_all.tsv\n",
      "filtering Matrix_chile_GEN_M1_all.tsv\n",
      "filtering Matrix_chile_GEN_guidi_all.tsv\n",
      "filtering Matrix_chile_GEN_salazar_all.tsv\n",
      "filtering Matrix_chile_GEN_stress_all.tsv\n",
      "filtering Matrix_chile_GEN_M0_all.tsv\n",
      "filtering Matrix_chile_GEN_M1_all.tsv\n",
      "filtering Matrix_chile_GEN_guidi_all.tsv\n",
      "filtering Matrix_chile_GEN_salazar_all.tsv\n",
      "filtering Matrix_chile_GEN_stress_all.tsv\n"
     ]
    }
   ],
   "source": [
    "depth_list = ['SRF','EPI','MES']\n",
    "for depth in depth_list:\n",
    "    md_clean = md[md['Depth level'] == depth]\n",
    "    # Read matrices of interest and sort them alphabetically\n",
    "    files = os.listdir(md_dir)\n",
    "    matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('_all.tsv')])\n",
    "    for name in matrix_files:\n",
    "        print(f\"filtering {name}\")\n",
    "        file_path = os.path.join(md_dir, name)\n",
    "        matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "        clean_matrix = md_clean.join(matrix).drop(md_clean.columns,axis = 1)\n",
    "        output_filename = '_'.join(name.split('_')[:-1]) + f'_{depth.lower()}.tsv'\n",
    "        clean_matrix.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing k-means to Matrix_chile_GEN_M0_srf.tsv\n",
      "performing k-means to Matrix_chile_GEN_M1_srf.tsv\n",
      "performing k-means to Matrix_chile_GEN_guidi_srf.tsv\n",
      "performing k-means to Matrix_chile_GEN_salazar_srf.tsv\n",
      "performing k-means to Matrix_chile_GEN_stress_srf.tsv\n",
      "performing k-means to Matrix_chile_GEN_M0_epi.tsv\n",
      "performing k-means to Matrix_chile_GEN_M1_epi.tsv\n",
      "performing k-means to Matrix_chile_GEN_guidi_epi.tsv\n",
      "performing k-means to Matrix_chile_GEN_salazar_epi.tsv\n",
      "performing k-means to Matrix_chile_GEN_stress_epi.tsv\n",
      "performing k-means to Matrix_chile_GEN_M0_mes.tsv\n",
      "performing k-means to Matrix_chile_GEN_M1_mes.tsv\n",
      "performing k-means to Matrix_chile_GEN_guidi_mes.tsv\n",
      "performing k-means to Matrix_chile_GEN_salazar_mes.tsv\n",
      "performing k-means to Matrix_chile_GEN_stress_mes.tsv\n"
     ]
    }
   ],
   "source": [
    "input_dir = '../01_data/01_biological_data/tara_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "\n",
    "\n",
    "# Perform K-Means for different n-clusters for each matrix\n",
    "for layer in depth_list:\n",
    "    all_metrics_results = []\n",
    "    clustering_results_dict = {}\n",
    "    output_dir = f'../03_results/out_genomic_clusters/clusters_ch/bio_clusters/bio_clusters_{layer.lower()}'\n",
    "    matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith(f'_{layer.lower()}.tsv')])\n",
    "    n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "    for matrix_file in matrix_files:\n",
    "        print(f\"performing k-means to {matrix_file}\")\n",
    "        file_path = os.path.join(input_dir, matrix_file)\n",
    "        matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "        base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        matrix_type_subsample = \"_\".join(base_filename.split('_')[3:])\n",
    "        \n",
    "        perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "        # CLR normalized matrix clustering\n",
    "        clr_matrix = clr_(matrix)\n",
    "        perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "\n",
    "\n",
    "    combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "    #combined_clustering_results = combined_clustering_results.sort_index(axis=1)\n",
    "\n",
    "    # Results of the kmeans\n",
    "    output_filename = f'kmeans_results_ch_{layer}.tsv'\n",
    "    combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "    # Results of the metrics of the kmeans clustering\n",
    "    metrics_df = pd.DataFrame(all_metrics_results)\n",
    "    metrics_output_filename = f'kmeans_metrics_ch_{layer}.tsv'\n",
    "    metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "    # Plot metrics\n",
    "    unique_matrices = metrics_df['matrix'].unique()\n",
    "    for matrix_type_subsample in unique_matrices:\n",
    "        matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "        \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "        ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "        ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "        ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 60))\n",
    "        ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "        ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "        ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "        ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "        ax4 = ax1.twinx()\n",
    "        ax4.spines['right'].set_position(('outward', 120))\n",
    "        ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "        ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "        ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f'kmeans_metrics_ch_{matrix_type_subsample}.pdf'\n",
    "        plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_kmeans_dir = '..\\\\03_results\\\\out_genomic_clusters\\\\clusters_ch\\\\bio_clusters'\n",
    "df_list = []\n",
    "for fold in os.listdir(input_kmeans_dir):\n",
    "    folder = f\"{input_kmeans_dir}/{fold}\"\n",
    "    layer = folder.split('_')[-1]\n",
    "    if layer == 'all':\n",
    "        layer = ''\n",
    "    else:\n",
    "        layer = '_'+layer\n",
    "    path = f\"{folder}/kmeans_results_ch{layer.upper()}.tsv\"\n",
    "    df = pd.read_csv(path, sep = '\\t')\n",
    "    \n",
    "    df_list.append(df.set_index('Samples'))\n",
    "\n",
    "all_clusters = pd.concat(df_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sat_dir = '../01_data/02_satellite_data_processed'\n",
    "\n",
    "desired_files = [\n",
    "'matrix_tara_chile_adj_grids_25_all.tsv'\n",
    "]\n",
    "\n",
    "predictor_files = sorted([f for f in glob(os.path.join(input_sat_dir, 'matrix_tara_chile_adj_grids_*.tsv')) \n",
    "                          if os.path.basename(f) in desired_files])\n",
    "\n",
    "target_vars = all_clusters\n",
    "target_vars = target_vars.map(lambda x: f\"C{x}\")\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'} # only consider this number of clusters # only consider clr-abundance clusters\n",
    "\n",
    "df_cols = []\n",
    "for col in df_list[0].columns:\n",
    "    bits = col.split('_')\n",
    "    if bits[0]=='clr' and bits[-1] in desired_clusters:\n",
    "        new_col = '_'.join([b for b in bits if b not in ['all','srf','mes','epi']])\n",
    "        df_cols.append(new_col)\n",
    "\n",
    "results_df_bd = pd.DataFrame(columns = df_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = input_kmeans_dir\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for fold in os.listdir(input_kmeans_dir):\n",
    "    folder = f\"{input_kmeans_dir}/{fold}\"\n",
    "    layerr = folder.split('_')[-1]\n",
    "    if layerr == 'all':\n",
    "        layer = ''\n",
    "    else:\n",
    "        layer = '_'+layerr\n",
    "    \n",
    "    path = f\"{folder}/kmeans_results_ch{layer.upper()}.tsv\"\n",
    "    target_vars = pd.read_csv(path, sep = '\\t', index_col= 0)\n",
    "    columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters]\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    for col in columns_to_use:\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "        bits = col.split('_')\n",
    "        score_col = '_'.join([b for b in bits if b not in ['all','srf','mes','epi']])\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, col]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df_bd.at[layer, score_col] = avg_f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = np.where(results_df_bd == results_df_bd.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_bd.to_csv(path_or_buf= '../03_results/out_predictions/predictions_kmeans_ch_bio.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clr_M1_kmeans_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>_srf</th>\n",
       "      <td>0.941944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clr_M1_kmeans_5\n",
       "_srf        0.941944"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_bd.iloc[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata-based bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation and Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/tara_ch/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df = md_df[['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]', 'Depth level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata quantile-based binning\n",
    "n_bins = [3,4,5,6,7,8] \n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all','SRF','EPI','MES']\n",
    "for n in n_bins:\n",
    "    q = 1/n\n",
    "    for layer in layers:\n",
    "        if layer == 'all':\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[feat] \n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df[binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)\n",
    "        else:\n",
    "            for feat in feats:\n",
    "                clean_feat = feat.split(\" \", 1)[0]\n",
    "                binning = f\"{clean_feat}_{n}_{layer}\"\n",
    "                data = md_df[md_df['Depth level'] == layer][feat]\n",
    "                ratios = [k*q for k in range(1,n)]\n",
    "                k_list = list(range(len(ratios)))\n",
    "                k_list.reverse()\n",
    "                quantiles = data.quantile(ratios).to_list()\n",
    "                quantiles.reverse()\n",
    "                quantiles = zip(k_list, quantiles)\n",
    "                md_df.loc[md_df['Depth level'] == layer,binning] = len(k_list)\n",
    "                for k, quant in quantiles:\n",
    "                    for ind in data.index:\n",
    "                        val = data[ind]\n",
    "                        if val<= quant:\n",
    "                            md_df.at[ind,binning] = int(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers:\n",
    "    if layer == 'all':\n",
    "        data = md_df[[col for col in md_df.columns if layer in col]]\n",
    "    else:\n",
    "        data = md_df[md_df['Depth level'] == layer][[col for col in md_df.columns if layer in col]]\n",
    "    data.to_csv(path_or_buf= f'../03_results/clusters_ch/metadata_based_clusters/metadata_clusters_{layer}.tsv', sep= '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "cluster_dir = '../03_results/out_genomic_clusters/clusters_ch/metadata_based_clusters'\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "feats = ['Temperature [ºC]','Oxygen [ml/l]','nitrates [uM]']\n",
    "layers = ['all''SRF','EPI','MES']\n",
    "columns_to_use = []\n",
    "for feat in feats:\n",
    "    clean_feat = feat.split(\" \", 1)[0]\n",
    "    for n in desired_clusters:\n",
    "        columns_to_use.append(clean_feat+'_'+n)\n",
    "\n",
    "results_df_md = pd.DataFrame(index=layers, columns=columns_to_use)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "\n",
    "for target_vars_filename in [f for f in os.listdir(cluster_dir) if not f.split('_')[-1] == 'metrics.tsv']:\n",
    "    target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "    target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    layer = target_vars_filename[-7:-4]\n",
    "    for col in columns_to_use:\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        feat = col.split('_')[0]\n",
    "        target_column = f\"{feat}_{n_clusters}_{layer}\"\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, target_column]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df_md.at[layer, col] = avg_f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = np.where(results_df_md == results_df_md.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SRF</th>\n",
       "      <td>0.901124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Temperature_7\n",
       "SRF      0.901124"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_md.iloc[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_md.to_csv(path_or_buf='../03_results/out_predictions/predictions_mdbins.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-bio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and clusters \n",
    "md_path = '../01_data/01_biological_data/tara_ch/metadata_chile.tsv'\n",
    "md_df = pd.read_csv(md_path, sep = \"\\t\")\n",
    "\n",
    "# Prepare df for the study\n",
    "md_df.set_index('Samples', inplace=True)\n",
    "usable_cols = [i for i in md_df.columns if md_df.dtypes.loc[i] in [float,int]]\n",
    "cols = [c for c in usable_cols if md_df.min()[c]>0]\n",
    "md_df = md_df[cols]\n",
    "s1 = md_df['Nitrate [uM]']\n",
    "s2 = md_df['Nitrates [uM]']\n",
    "nitrates = 0.5*(s1+s2)\n",
    "\n",
    "md_df['nitrates [uM]'] = nitrates \n",
    "\n",
    "md_df.drop(columns=['Nitrate [uM]','Nitrates [uM]'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dataframes based on which the clusters will be made, taking care in eliminating unnecessary columns. For that, we firstly drop all the non-important technical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_path = '../01_data/01_biological_data/tara_ch'\n",
    "path_list = [path for path in os.listdir(bio_path) if 'Matrix_chile' in path and 'srf.tsv' in path and path != 'metadata_clile.tsv']\n",
    "df_list = []\n",
    "for path in path_list:\n",
    "    full_path = f\"{bio_path}/{path}\"\n",
    "    bio_df = pd.read_csv(full_path, sep = '\\t').set_index('Samples')\n",
    "    full_df = pd.concat([md_df.loc[md_df.index.intersection(bio_df.index)], bio_df], axis =1)\n",
    "    df = full_df.drop(columns=[c for c in md_df.columns if c in ['Leg', 'Station', 'Station ID', 'Depth ID', 'lat_cast',\n",
    "       'lon_cast', 'datetime', 'Depth [m]', 'instrument','original file', 'year', 'month', 'day', 'hour', 'minute',\n",
    "       'second']])\n",
    "    ordered = df.nunique().sort_values().copy(deep = True)\n",
    "    for key in tqdm(ordered.index):\n",
    "        if ordered[key] > 1:\n",
    "            break\n",
    "    first_non_triv_key = key\n",
    "    first_non_triv_ind = ordered.index.get_loc(first_non_triv_key)\n",
    "    triv_keys = ordered.index[:first_non_triv_ind]\n",
    "    df.drop(columns = triv_keys, inplace = True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "\n",
    "all_metrics_results = []\n",
    "clustering_results_dict = {}\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Means for different n-clusters for each matrix\n",
    "\n",
    "layers = ['all','srf','mes','epi']\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "bio_path = '../01_data/01_biological_data/tara_ch'\n",
    "for layer in layers:\n",
    "    output_dir = f'../03_results/out_genomic_clusters/clusters_ch/metabio_clusters/{layer}'\n",
    "    all_metrics_results = []\n",
    "    clustering_results_dict = {}\n",
    "    path_list = [path for path in os.listdir(bio_path) if 'Matrix_chile' in path and f'{layer}.tsv' in path and path != 'metadata_clile.tsv']\n",
    "    for name in path_list:\n",
    "        full_path = f\"{bio_path}/{name}\"\n",
    "        bio_df = pd.read_csv(full_path, sep = '\\t').set_index('Samples')\n",
    "        full_df = pd.concat([md_df.loc[md_df.index.intersection(bio_df.index)], bio_df], axis =1)\n",
    "        df = full_df.drop(columns=[c for c in md_df.columns if c in ['Leg', 'Station', 'Station ID', 'Depth ID', 'lat_cast',\n",
    "        'lon_cast', 'datetime', 'Depth [m]', 'instrument','original file', 'year', 'month', 'day', 'hour', 'minute',\n",
    "        'second']])\n",
    "        ordered = df.nunique().sort_values().copy(deep = True)\n",
    "        for key in tqdm(ordered.index):\n",
    "            if ordered[key] > 1:\n",
    "                break\n",
    "        first_non_triv_key = key\n",
    "        first_non_triv_ind = ordered.index.get_loc(first_non_triv_key)\n",
    "        triv_keys = ordered.index[:first_non_triv_ind]\n",
    "        df.drop(columns = triv_keys, inplace = True)\n",
    "        matrix = df.copy(deep = True).dropna(axis = 0) \n",
    "        matrix_type_subsample = name[17:-4]\n",
    "        perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "        # CLR normalized matrix clustering\n",
    "        clr_matrix = clr_(matrix)\n",
    "        perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "\n",
    "    combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "\n",
    "    # Results of the kmeans\n",
    "    output_filename = f'kmeans_results_metabio_ch_{layer}.tsv'\n",
    "    combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "    # Results of the metrics of the kmeans clustering\n",
    "    metrics_df = pd.DataFrame(all_metrics_results)\n",
    "    metrics_output_filename = f'kmeans_metrics_metabio_ch_{layer}.tsv'\n",
    "    metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "    # Plot metrics\n",
    "    unique_matrices = metrics_df['matrix'].unique()\n",
    "    print('Plotting metrics.')\n",
    "    for matrix_type_subsample in tqdm(unique_matrices):\n",
    "        print(matrix_type_subsample)\n",
    "        matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "        \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax1.set_xlabel('Number of Clusters')\n",
    "        ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "        ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "        ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "        ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "        ax3 = ax1.twinx()\n",
    "        ax3.spines['right'].set_position(('outward', 60))\n",
    "        ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "        ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "        ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "        ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "        ax4 = ax1.twinx()\n",
    "        ax4.spines['right'].set_position(('outward', 120))\n",
    "        ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "        ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "        ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "        # Save the plot\n",
    "        plot_filename = f'kmeans_metrics_{matrix_type_subsample}_metabio_ch.pdf'\n",
    "        plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m         df_columns\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m results_df_mb \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(index \u001b[38;5;241m=\u001b[39m layers, columns \u001b[38;5;241m=\u001b[39m df_columns)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(layers):\n\u001b[0;32m     33\u001b[0m     cluster_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../03_results/out_genomic_clusters/clusters_ch/metabio_clusters/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m     target_vars_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans_results_metabio_ch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "predictors = pd.read_csv('../01_data/02_satellite_data_processed/matrix_tara_chile_adj_grids_25_all.tsv',sep = '\\t').set_index('Samples')\n",
    "\n",
    "desired_clusters = {'5', '6', '7', '8'}\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    #recall = recall_score(y_true, y_pred, average='macro')\n",
    "    #precision = precision_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    #roc_auc = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    return (accuracy, f1)\n",
    "\n",
    "n_splits = 8\n",
    "n_repeats = 9\n",
    "\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_macro': make_scorer(f1_score, average='macro')\n",
    "}\n",
    "layers = ['all','srf','epi','mes']\n",
    "matrix_types = ['M0','M1','stress','guidi','salazar']\n",
    "df_columns = []\n",
    "for n in desired_clusters:\n",
    "    for type in matrix_types:\n",
    "        df_columns.append(f\"clr_{type}_{n}\")\n",
    "results_df_mb = pd.DataFrame(index = layers, columns = df_columns)\n",
    "\n",
    "for layer in tqdm(layers):\n",
    "    cluster_dir = f'../03_results/out_genomic_clusters/clusters_ch/metabio_clusters/{layer}'\n",
    "    target_vars_filename = f'kmeans_results_metabio_ch_{layer}.tsv'\n",
    "    target_vars_path = os.path.join(cluster_dir, target_vars_filename)\n",
    "    print(f\"Reading {target_vars_path}\")\n",
    "    #DataFrame with the clusters\n",
    "    target_vars = pd.read_csv(target_vars_path, sep='\\t', index_col=0)\n",
    "\n",
    "    columns_to_use = [col for col in target_vars.columns if col.startswith('clr_') and col.split('_')[-1] in desired_clusters] # only consider clr-abundance clusters\n",
    "\n",
    "    aligned_predictor = predictors.loc[predictors.index.intersection(target_vars.index)]\n",
    "    for col in columns_to_use:\n",
    "        bits = col.split('_')\n",
    "        mat_type = bits[1]\n",
    "        rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=0)\n",
    "\n",
    "        scoring = {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'f1_macro': make_scorer(f1_score, average='macro')\n",
    "        }\n",
    "\n",
    "        n_clusters = int(col.split('_')[-1])\n",
    "        col_tag = f\"clr_{mat_type}_{n_clusters}\"\n",
    "        X = aligned_predictor\n",
    "        y = target_vars.loc[aligned_predictor.index, col]\n",
    "        non_nan_indices = y.dropna().index\n",
    "        X = X.loc[non_nan_indices]\n",
    "        y = y.loc[non_nan_indices]\n",
    "        \n",
    "        y_encoded = le.fit_transform(y)\n",
    "        unique, counts = np.unique(y_encoded, return_counts=True)\n",
    "        min_samples = n_splits\n",
    "\n",
    "        X_resampled = X.copy()\n",
    "        y_resampled = y_encoded.copy()\n",
    "\n",
    "        for cls, count in zip(unique, counts):\n",
    "            if count < min_samples:\n",
    "                diff = min_samples - count\n",
    "                cls_indices = np.where(y_encoded == cls)[0]\n",
    "                indices_to_duplicate = np.random.choice(cls_indices, diff, replace=True)\n",
    "                X_resampled = np.concatenate([X_resampled, X.iloc[indices_to_duplicate]], axis=0)\n",
    "                y_resampled = np.concatenate([y_resampled, y_encoded[indices_to_duplicate]], axis=0)\n",
    "\n",
    "        model = xgb.XGBClassifier(eval_metric='merror', \n",
    "                                    seed = 29,\n",
    "                                    objective= 'multi: softmax',\n",
    "                                    num_class = n_clusters,\n",
    "                                    learning_rate =0.2,\n",
    "                                    n_estimators=10,\n",
    "                                    max_depth=5,\n",
    "                                    min_child_weight=1,\n",
    "                                    gamma=0,\n",
    "                                    subsample=0.8,\n",
    "                                    colsample_bytree=0.8\n",
    "                                    )\n",
    "\n",
    "        #cv_results = cross_validate(model, X, y_encoded, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "        cv_results = cross_validate(model, X_resampled, y_resampled, cv=rskf, scoring=scoring, return_train_score=False)\n",
    "\n",
    "        avg_accuracy = np.mean(cv_results['test_accuracy'])\n",
    "        avg_f1_macro = np.mean(cv_results['test_f1_macro'])\n",
    "\n",
    "        results_df_mb.at[layer, col_tag] = f\"({avg_accuracy}, {avg_f1_macro})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = np.where(results_df_mb == results_df_mb.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mb.iloc[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_mb.to_csv(path_or_buf = '../03_results/out_predictions/predictions_kmeans_metabio.tsv',sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
