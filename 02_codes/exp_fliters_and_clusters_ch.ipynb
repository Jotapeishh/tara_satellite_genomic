{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparativa de técnicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to copare two tecniques of clustering applied on `SRF` level samples.\n",
    "* Take the old clusterizations, filtering the `SRF` samples, and mapping. \n",
    "* Filter, cluster, and then map. \n",
    "\n",
    "The filename will signal this following this nomenclature:\n",
    "* `_cf`: clustered filtered.\n",
    "* `_fc`: filtered clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we will make use of the `plot_clusters_on_map` function defined in the `clustering_projection_map` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_clusters_on_map(merged_data, cluster_column):\n",
    "    filtered_data = merged_data[~merged_data[cluster_column].isna()] # filter out rows where cluster_column is NaN\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    axes = axes.flatten()  # flatten the array of axes for easy iteration\n",
    "    \n",
    "    plot_titles = [\n",
    "        f'Clusters Projection: {cluster_column}',\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    data_columns = [\n",
    "        cluster_column,\n",
    "        'Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio'\n",
    "    ]\n",
    "    \n",
    "    unique_clusters = filtered_data[cluster_column].unique()\n",
    "    num_clusters = len(unique_clusters)\n",
    "    marker_styles = ['o', 's', '^', 'v', '<', '>', 'd', 'p', 'h', 'H', '*', 'x', '+', 'D']\n",
    "    if num_clusters > len(marker_styles):\n",
    "        marker_styles = (marker_styles * ((num_clusters // len(marker_styles)) + 1))[:num_clusters]\n",
    "    cluster_marker_map = dict(zip(unique_clusters, marker_styles))\n",
    "    \n",
    "    env_vars = ['Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']\n",
    "    norms = {}\n",
    "    \n",
    "    for data_column in env_vars:\n",
    "        vmin = filtered_data[data_column].min()\n",
    "        vmax = filtered_data[data_column].max()\n",
    "        norms[data_column] = Normalize(vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.set_extent([-80, -67, -55,-17])\n",
    "\n",
    "        ax.add_feature(cfeature.LAND)\n",
    "        ax.add_feature(cfeature.OCEAN)\n",
    "        ax.add_feature(cfeature.BORDERS)\n",
    "        \n",
    "        ax.set_title(plot_titles[idx])\n",
    "        \n",
    "        data_column = data_columns[idx]\n",
    "        plot_data = filtered_data[~filtered_data[data_column].isna()]\n",
    "        \n",
    "        if idx == 0:\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    label=f'Cluster {cluster_id}',\n",
    "                    s=35,\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            ax.legend(loc='upper left')\n",
    "        else:\n",
    "            norm = norms[data_column]\n",
    "            for cluster_id in unique_clusters:\n",
    "                cluster_points = plot_data[plot_data[cluster_column] == cluster_id]\n",
    "                sc = ax.scatter(\n",
    "                    cluster_points['lon_cast'],\n",
    "                    cluster_points['lat_cast'],\n",
    "                    c=cluster_points[data_column],\n",
    "                    s=35,\n",
    "                    cmap='viridis',\n",
    "                    marker=cluster_marker_map[cluster_id],\n",
    "                    edgecolors='black',\n",
    "                    norm=norm,\n",
    "                    transform=ccrs.PlateCarree()\n",
    "                )\n",
    "            cbar = plt.colorbar(sc, ax=ax, orientation='vertical', shrink=0.5)\n",
    "            cbar.set_label(data_column)\n",
    "            handles = []\n",
    "            for cluster_id in unique_clusters:\n",
    "                marker = cluster_marker_map[cluster_id]\n",
    "                handle = plt.Line2D([], [], color='black', marker=marker, linestyle='', markersize=8, label=f'Cluster {cluster_id}')\n",
    "                handles.append(handle)\n",
    "            ax.legend(handles=handles, loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'clusters_{cluster_column}.pdf')\n",
    "    plt.savefig(output_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.- Cluster -> Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data generation.\n",
    "This code right ahead takes all the already clustered data, and saves a file of the selection of those samples that are 'SRF'. If a file named `kmeans_results_ch_srf_clustered_filtered.tsv` exists already in the folder, there is no need to run this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch.tsv'\n",
    "\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in matrix_files:\n",
    "    pth = f\"{input_dir}/{file}\"\n",
    "    #bio_mtrx es en verdad cluster_mtrx. Arreglar para legibilidad.\n",
    "    clstr_mtrx =  pd.read_csv(file_path, sep='\\t', index_col=0) \n",
    "    meta_mtrx = pd.read_csv('../01_data/01_biological_data/metadata_chile.tsv', sep='\\t', index_col=0) \n",
    "    meta_mtrx = meta_mtrx[meta_mtrx['Depth level']== 'SRF']\n",
    "    dirty_df = meta_mtrx.join(clstr_mtrx)\n",
    "    clean_df = dirty_df.drop(meta_mtrx.columns, axis=1)\n",
    "    new_keys = {col: col+'_cf' for col in clean_df.columns}\n",
    "    clean_df.rename(columns = new_keys, inplace=True)\n",
    "    output_filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "    clean_df.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_srf_clustered_filtered.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.- Filter -> Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "We start by collecting the bio data, filtering only the SRF samples, and saving the resulting matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_dir = '../01_data/01_biological_data'\n",
    "md_filename = 'metadata_chile.tsv'\n",
    "md_path = os.path.join(md_dir,md_filename)\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n",
    "\n",
    "output_dir = md_dir \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_clean = md[md['Depth level']=='SRF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(md_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('.tsv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in matrix_files:\n",
    "    print(f\"filtering {name}\")\n",
    "    file_path = os.path.join(md_dir, name)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    clean_matrix = md_clean.join(matrix).drop(md_clean.columns,axis = 1)\n",
    "    output_filename =  name[:-8] + '_srf.tsv'\n",
    "    clean_matrix.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLR implementation\n",
    "def clr_(data, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Perform centered log-ratio (clr) normalization on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (pandas.DataFrame): A DataFrame with samples as rows and components as columns.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A clr-normalized DataFrame.\n",
    "    \"\"\"\n",
    "    if (data < 0).any().any():\n",
    "        raise ValueError(\"Data should be strictly positive for clr normalization.\")\n",
    "\n",
    "    # Add small amount to cells with a value of 0\n",
    "    if (data <= 0).any().any():\n",
    "        data = data.replace(0, eps)\n",
    "\n",
    "    # Calculate the geometric mean of each row\n",
    "    gm = np.exp(data.apply(np.log).mean(axis=1))\n",
    "\n",
    "    # Perform clr transformation\n",
    "    clr_data = data.apply(np.log).subtract(np.log(gm), axis=0)\n",
    "\n",
    "    return clr_data\n",
    "\n",
    "all_metrics_results = []\n",
    "clustering_results_dict = {}\n",
    "\n",
    "def perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False):\n",
    "    suffix = 'clr_' if clr else ''\n",
    "    # Perform K-Means for different 'n'\n",
    "    for n_clusters in n_clusters_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=50)\n",
    "        kmeans.fit(matrix)\n",
    "        \n",
    "        cluster_labels = kmeans.labels_\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        inertia = kmeans.inertia_\n",
    "        silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
    "        davies_bouldin = davies_bouldin_score(matrix, cluster_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(matrix, cluster_labels)\n",
    "        \n",
    "        all_metrics_results.append({\n",
    "            'matrix': f\"{suffix}{matrix_type_subsample}\",\n",
    "            'n_clusters': n_clusters,\n",
    "            'inertia': inertia,\n",
    "            'silhouette_score': silhouette_avg,\n",
    "            'davies_bouldin_score': davies_bouldin,\n",
    "            'calinski_harabasz_score': calinski_harabasz\n",
    "        })\n",
    "        \n",
    "        col_name = f\"{suffix}{matrix_type_subsample}_kmeans_{n_clusters}\" # Create a DataFrame for the cluster labels with appropriate column names\n",
    "        results = pd.DataFrame({col_name: cluster_labels}, index=matrix.index)\n",
    "        \n",
    "        if col_name not in clustering_results_dict:\n",
    "            clustering_results_dict[col_name] = results\n",
    "        else:\n",
    "            clustering_results_dict[col_name] = pd.concat([clustering_results_dict[col_name], results], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../01_data/01_biological_data'\n",
    "output_dir = '../03_results/out_genomic_clusters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read matrices of interest and sort them alphabetically\n",
    "files = os.listdir(input_dir)\n",
    "matrix_files = sorted([f for f in files if f.startswith('Matrix_chile_GEN_') and f.endswith('_srf.tsv')])\n",
    "\n",
    "# Perform K-Means for different n-clusters for each matrix\n",
    "n_clusters_list = [3, 4, 5, 6, 7, 8]\n",
    "for matrix_file in matrix_files:\n",
    "    print(f\"performing k-means to {matrix_file}\")\n",
    "    file_path = os.path.join(input_dir, matrix_file)\n",
    "    matrix = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    matrix_type_subsample = \"_\".join(base_filename.split('_')[3:])\n",
    "    \n",
    "    perform_kmeans_clustering(matrix, matrix_type_subsample, n_clusters_list, clr=False)\n",
    "    # CLR normalized matrix clustering\n",
    "    clr_matrix = clr_(matrix)\n",
    "    perform_kmeans_clustering(clr_matrix, matrix_type_subsample, n_clusters_list, clr=True)\n",
    "\n",
    "\n",
    "\n",
    "combined_clustering_results = pd.concat(clustering_results_dict.values(), axis=1)\n",
    "#combined_clustering_results = combined_clustering_results.sort_index(axis=1)\n",
    "\n",
    "# Results of the kmeans\n",
    "output_filename = 'kmeans_results_ch_fc.tsv'\n",
    "combined_clustering_results.to_csv(os.path.join(output_dir, output_filename), sep='\\t', index=True)\n",
    "\n",
    "# Results of the metrics of the kmeans clustering\n",
    "metrics_df = pd.DataFrame(all_metrics_results)\n",
    "metrics_output_filename = 'kmeans_metrics_ch_fc.tsv'\n",
    "metrics_df.to_csv(os.path.join(output_dir, metrics_output_filename), sep='\\t', index=False)\n",
    "\n",
    "# Plot metrics\n",
    "unique_matrices = metrics_df['matrix'].unique()\n",
    "for matrix_type_subsample in unique_matrices:\n",
    "    matrix_metrics_df = metrics_df[metrics_df['matrix'] == matrix_type_subsample]\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "    ax1.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['inertia'], color='tab:blue', label='Inertia')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Silhouette Score', color='tab:orange')\n",
    "    ax2.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['silhouette_score'], color='tab:orange', label='Silhouette Score')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "    ax2.axhline(y=0.25, color='tab:orange', linestyle='--', linewidth=1, label='Silhouette Score Threshold (0.25)')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    ax3.set_ylabel('Davies-Bouldin Score', color='tab:green')\n",
    "    ax3.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['davies_bouldin_score'], color='tab:green', label='Davies-Bouldin Score')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax3.axhline(y=1.50, color='tab:green', linestyle='--', linewidth=1, label='Davies-Bouldin Score Threshold (1.50)')\n",
    "\n",
    "    ax4 = ax1.twinx()\n",
    "    ax4.spines['right'].set_position(('outward', 120))\n",
    "    ax4.set_ylabel('Calinski-Harabasz Score', color='tab:red')\n",
    "    ax4.plot(matrix_metrics_df['n_clusters'], matrix_metrics_df['calinski_harabasz_score'], color='tab:red', label='Calinski-Harabasz Score')\n",
    "    ax4.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.title(f'Evaluation Metrics for {matrix_type_subsample}')\n",
    "\n",
    "    # Save the plot\n",
    "    plot_filename = f'kmeans_metrics_{matrix_type_subsample}_ch_fc.pdf'\n",
    "    plt.savefig(os.path.join(output_dir, plot_filename), bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../03_results/out_genomic_clusters'\n",
    "filename = 'kmeans_results_ch_fc.tsv'\n",
    "\n",
    "env_data_dir = '../01_data/01_biological_data'\n",
    "env_filename = 'metadata_chile.tsv'\n",
    "\n",
    "output_dir = '../03_results/out_genomic_clusters/map_projections_ch'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(input_dir,filename)\n",
    "md_path = os.path.join(env_data_dir,env_filename)\n",
    "\n",
    "clusters = pd.read_csv(file_path, sep='\\t', index_col=0)\n",
    "new_keys = {col: col+'_fc' for col in clusters.columns}\n",
    "clusters.rename(columns = new_keys, inplace=True)\n",
    "\n",
    "\n",
    "for col in clusters.columns:\n",
    "    clusters[col] = clusters[col].astype('Int64')\n",
    "md = pd.read_csv(md_path, sep='\\t', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_selection = ['lat_cast','lon_cast','Temperature [ºC]',\n",
    "       'Salinity [PSU]', 'Oxygen [%]',\n",
    "       'Fluorescence [mg/m3]', 'Orthophosphate [uM]', 'Silicic-acid [uM]',\n",
    "       'Nitrite [uM]', 'Nitrates [uM]', 'NP ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = clusters.join(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in clusters.columns:\n",
    "    plot_clusters_on_map(merged_data, column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
